# ğŸ“š Machine Learning Mathematics Guide
*Complete Mathematical Foundations for ML Framework*

## ğŸ¯ Overview

This guide provides comprehensive mathematical explanations for all algorithms and concepts used in our ML framework. Each section includes the mathematical formulation, intuitive explanation, and practical implementation details.

---

## ğŸ“Š 1. AdaBoost (Adaptive Boosting)

### ğŸ§® Mathematical Foundation

#### AdaBoost Algorithm
AdaBoost combines multiple weak learners sequentially, where each learner focuses on previously misclassified examples.

**Core Algorithm Steps:**

1. **Initialize sample weights:**
```
wâ‚(i) = 1/N  for i = 1, ..., N
```

2. **For each boosting round t = 1, ..., T:**

   a) **Train weak learner hâ‚œ with weighted samples**
   
   b) **Calculate weighted error:**
   ```
   Îµâ‚œ = âˆ‘áµ¢ wâ‚(i) Â· I(hâ‚œ(xáµ¢) â‰  yáµ¢)
   ```
   
   c) **Calculate classifier weight:**
   ```
   Î±â‚› = (1/2) Â· ln((1 - Îµâ‚œ)/Îµâ‚œ)
   ```
   
   d) **Update sample weights:**
   ```
   wâ‚œâ‚Šâ‚(i) = wâ‚œ(i) Â· exp(-Î±â‚› Â· yáµ¢ Â· hâ‚œ(xáµ¢)) / Zâ‚œ
   ```
   where Zâ‚œ is normalization factor

3. **Final classifier:**
```
H(x) = sign(âˆ‘â‚œâ‚Œâ‚áµ€ Î±â‚œ Â· hâ‚œ(x))
```

#### Key Mathematical Insights:

**Weight Update Mechanism:**
- **Correctly classified**: wâ‚œâ‚Šâ‚(i) = wâ‚œ(i) Â· exp(-Î±â‚œ) (weight decreases)
- **Misclassified**: wâ‚œâ‚Šâ‚(i) = wâ‚œ(i) Â· exp(Î±â‚œ) (weight increases)
- **Effect**: Forces next classifier to focus on difficult examples

**Classifier Weight (Î±â‚œ):**
- **Low error (Îµâ‚œ â†’ 0)**: Î±â‚œ â†’ âˆ (high influence)
- **High error (Îµâ‚œ â†’ 0.5)**: Î±â‚œ â†’ 0 (low influence)
- **Random guessing (Îµâ‚œ = 0.5)**: Î±â‚œ = 0 (no contribution)

### ğŸ¯ Key Parameters Explained

#### n_estimators (Number of Weak Learners)
```
Mathematical Impact: H(x) = sign(âˆ‘â‚œâ‚Œâ‚â¿_áµ‰Ë¢áµ—â±áµáµƒáµ—áµ’Ê³Ë¢ Î±â‚œ Â· hâ‚œ(x))
```
- **Small values (10-50)**: Fast training, potential underfitting
- **Large values (100-500)**: Better performance, overfitting risk
- **Trade-off**: Bias vs. Variance, Training time vs. Accuracy

#### learning_rate (Shrinkage Parameter)
```
Modified classifier weight: Î±'â‚œ = learning_rate Ã— Î±â‚œ
Final prediction: H(x) = sign(âˆ‘â‚œâ‚Œâ‚áµ€ Î±'â‚œ Â· hâ‚œ(x))
```
- **High rates (1.0-2.0)**: Aggressive learning, fast convergence
- **Low rates (0.1-0.5)**: Conservative learning, better generalization
- **Mathematical effect**: Scales the contribution of each weak learner

#### algorithm Parameter
**SAMME (Stagewise Additive Modeling using Multi-class Exponential loss):**
```
Uses discrete class predictions: hâ‚œ(x) âˆˆ {-1, +1}
Weight update: standard AdaBoost formula
```

**SAMME.R (Real SAMME):**
```
Uses class probabilities: pâ‚œ(x) âˆˆ [0, 1]
Faster convergence through probability estimates
Better performance in most cases
```

### ğŸ” Loss Function Analysis

#### Exponential Loss (Classification)
```
L(y, f(x)) = exp(-y Â· f(x))
```
- **Correct prediction (yÂ·f(x) > 0)**: Loss < 1
- **Incorrect prediction (yÂ·f(x) < 0)**: Loss > 1
- **Property**: Exponentially penalizes misclassifications

#### Regression Loss Functions
**Linear Loss:**
```
L(y, f(x)) = |y - f(x)|
```

**Square Loss:**
```
L(y, f(x)) = (y - f(x))Â²
```

**Exponential Loss:**
```
L(y, f(x)) = exp(|y - f(x)|)
```

### ğŸ“ Theoretical Properties

#### Generalization Bound
```
P(error) â‰¤ âˆâ‚œâ‚Œâ‚áµ€ 2âˆš(Îµâ‚œ(1 - Îµâ‚œ))
```
- Shows exponential decrease in training error
- Explains AdaBoost's resistance to overfitting
- Connects weak learning to strong learning

#### Margin Theory
```
Margin(x) = (y Â· âˆ‘â‚œ Î±â‚œhâ‚œ(x)) / âˆ‘â‚œ Î±â‚œ
```
- AdaBoost tends to maximize margins
- Larger margins â†’ better generalization
- Explains continued improvement after zero training error

## ğŸ“Š 2. Support Vector Machines (SVM)

### ğŸ§® Mathematical Foundation

#### Linear SVM Optimization Problem
```
Minimize: (1/2)||w||Â² + Câˆ‘Î¾áµ¢

Subject to:
- yáµ¢(wÂ·xáµ¢ + b) â‰¥ 1 - Î¾áµ¢  for all i
- Î¾áµ¢ â‰¥ 0  for all i
```

**Where:**
- `w`: weight vector (defines the separating hyperplane)
- `b`: bias term (shifts the hyperplane)
- `C`: regularization parameter (controls trade-off)
- `Î¾áµ¢`: slack variables (allow for misclassification)
- `xáµ¢, yáµ¢`: training samples and labels

#### What Each Component Does:

**Objective Function: (1/2)||w||Â²**
- **Purpose**: Maximize margin between classes
- **Math**: ||w||Â² = wâ‚Â² + wâ‚‚Â² + ... + wâ‚™Â²
- **Intuition**: Smaller weights â†’ larger margin â†’ better generalization
- **Why 1/2**: Mathematical convenience for derivatives

**Regularization Term: Câˆ‘Î¾áµ¢**
- **Purpose**: Control tolerance for misclassification
- **High C**: Low tolerance (hard margin, might overfit)
- **Low C**: High tolerance (soft margin, might underfit)
- **Î¾áµ¢ = 0**: Point is correctly classified with sufficient margin
- **0 < Î¾áµ¢ < 1**: Point is correctly classified but within margin
- **Î¾áµ¢ â‰¥ 1**: Point is misclassified

### ğŸ¯ Kernel Functions

#### RBF (Radial Basis Function) Kernel
```
K(x, x') = exp(-Î³||x - x'||Â²)
```

**Mathematical Breakdown:**
- `||x - x'||Â²`: Squared Euclidean distance between points
- `Î³`: Kernel parameter (gamma)
  - High Î³: Tight, complex decision boundaries
  - Low Î³: Smooth, simple decision boundaries
- `exp()`: Exponential function creates smooth transitions

**How RBF Works:**
1. Calculate distance between every pair of points
2. Apply exponential decay: closer points â†’ higher similarity
3. Create infinite-dimensional feature space implicitly
4. Enable non-linear decision boundaries in original space

#### Polynomial Kernel
```
K(x, x') = (Î³âŸ¨x, x'âŸ© + r)áµˆ
```

**Parameters:**
- `d`: degree of polynomial (typically 2-4)
- `Î³`: scaling parameter
- `r`: independent term
- `âŸ¨x, x'âŸ©`: dot product of feature vectors

**Example (degree=2):**
- Original features: (xâ‚, xâ‚‚)
- Expanded features: (xâ‚Â², xâ‚‚Â², âˆš2xâ‚xâ‚‚, âˆš2xâ‚, âˆš2xâ‚‚, 1)
- Creates quadratic decision boundaries

### ğŸ”¢ SVM Decision Function
```
f(x) = sign(âˆ‘Î±áµ¢yáµ¢K(xáµ¢, x) + b)
```

**Components:**
- `Î±áµ¢`: Lagrange multipliers (learned during training)
- `yáµ¢`: class labels of support vectors
- `K(xáµ¢, x)`: kernel function
- `b`: bias term
- `sign()`: outputs +1 or -1 for classification

**Support Vectors:**
- Training points with Î±áµ¢ > 0
- Points that define the decision boundary
- Typically only 5-20% of training data
- Removing non-support vectors doesn't change the model

---

## ï¿½ 2. Naive Bayes

### ğŸ§® Mathematical Foundation

#### Bayes' Theorem
```
P(y|x) = P(x|y) Ã— P(y) / P(x)
```

**For Classification:**
```
P(class|features) = P(features|class) Ã— P(class) / P(features)
```

**Naive Independence Assumption:**
```
P(xâ‚, xâ‚‚, ..., xâ‚™|y) = P(xâ‚|y) Ã— P(xâ‚‚|y) Ã— ... Ã— P(xâ‚™|y)
```

### ğŸ¯ Gaussian Naive Bayes

#### Probability Density Function
```
P(xáµ¢|y) = (1/âˆš(2Ï€Ïƒáµ§Â²)) Ã— exp(-(xáµ¢ - Î¼áµ§)Â²/(2Ïƒáµ§Â²))
```

**Parameters (learned from training data):**
- `Î¼áµ§`: mean of feature xáµ¢ for class y
- `Ïƒáµ§Â²`: variance of feature xáµ¢ for class y

**Step-by-Step Calculation:**

1. **Learn Parameters (Training):**
   ```
   Î¼áµ§ = (1/náµ§) Ã— âˆ‘(xáµ¢ where yáµ¢ = y)
   Ïƒáµ§Â² = (1/náµ§) Ã— âˆ‘(xáµ¢ - Î¼áµ§)Â² where yáµ¢ = y
   ```

2. **Calculate Class Probabilities (Prediction):**
   ```
   P(y) = count(y) / total_samples
   ```

3. **Calculate Feature Likelihoods:**
   ```
   For each feature xáµ¢:
   P(xáµ¢|y) = gaussian_pdf(xáµ¢, Î¼áµ§, Ïƒáµ§)
   ```

4. **Combine Using Naive Assumption:**
   ```
   P(y|x) âˆ P(y) Ã— âˆP(xáµ¢|y)
   ```

**Why "Naive":**
- Assumes features are independent given the class
- Often violated in practice, but works surprisingly well
- Example: In text classification, word occurrences are not independent
- Despite violation, often achieves good performance

### ğŸ”¢ Prediction Formula
```
Å· = argmax P(y) Ã— âˆP(xáµ¢|y)
     y
```

**Practical Implementation (log probabilities):**
```
log P(y|x) = log P(y) + âˆ‘log P(xáµ¢|y)
```

**Why use log probabilities:**
- Avoids numerical underflow (multiplying many small numbers)
- Addition is faster than multiplication
- Monotonic transformation preserves ordering

---

## ğŸŒ³ 3. Ensemble Methods

### ğŸ¯ Random Forest Mathematics

#### Bagging (Bootstrap Aggregating)

**Bootstrap Sampling:**
- Create B bootstrap samples from original dataset
- Each sample: randomly select n samples with replacement
- Some samples appear multiple times, others not at all
- Reduces variance by averaging multiple models

**Random Feature Selection:**
- At each node split, randomly select m features from total p features
- Typical values: m = âˆšp for classification, m = p/3 for regression
- Reduces correlation between trees
- Improves generalization

#### Prediction Aggregation

**Classification (Majority Voting):**
```
Å· = mode{hâ‚(x), hâ‚‚(x), ..., hB(x)}
```

**Regression (Average):**
```
Å· = (1/B) Ã— âˆ‘háµ¦(x)
        b=1
```

**Out-of-Bag (OOB) Error:**
```
OOB Error = (1/n) Ã— âˆ‘I(yáµ¢ â‰  Å·áµ¢^OOB)
```
- Uses samples not included in bootstrap for validation
- No need for separate validation set
- Unbiased estimate of generalization error

### ğŸ¯ AdaBoost Mathematics

#### Adaptive Boosting Algorithm

**Weight Update Rule:**
```
wáµ¢^(t+1) = wáµ¢^(t) Ã— exp(-Î±â‚œyáµ¢hâ‚œ(xáµ¢)) / Zâ‚œ
```

**Where:**
- `wáµ¢^(t)`: weight of sample i at iteration t
- `Î±â‚œ`: weight of classifier t
- `hâ‚œ(xáµ¢)`: prediction of classifier t on sample i
- `Zâ‚œ`: normalization constant

**Classifier Weight:**
```
Î±â‚œ = (1/2) Ã— ln((1 - Îµâ‚œ)/Îµâ‚œ)
```

**Where:**
- `Îµâ‚œ`: weighted error rate of classifier t
- Higher accuracy â†’ higher weight in final prediction

**Final Prediction:**
```
H(x) = sign(âˆ‘Î±â‚œhâ‚œ(x))
        t=1
```

**Key Insights:**
- Misclassified samples get higher weights
- Next classifier focuses on difficult samples
- Sequentially reduces training error
- Can overfit if not regularized

---

## ğŸ“ 4. Performance Metrics

### ğŸ¯ Classification Metrics

#### Confusion Matrix Components
```
                Prediction
Actual    |  Pos  |  Neg  |
----------|-------|-------|
Positive  |  TP   |  FN   |
Negative  |  FP   |  TN   |
```

#### Metric Formulas

**Accuracy:**
```
Accuracy = (TP + TN) / (TP + TN + FP + FN)
```
- Overall correctness
- Good for balanced datasets
- Misleading for imbalanced datasets

**Precision:**
```
Precision = TP / (TP + FP)
```
- Of positive predictions, how many were correct?
- Important when false positives are costly
- Example: Spam detection (don't want to mark important emails as spam)

**Recall (Sensitivity):**
```
Recall = TP / (TP + FN)
```
- Of actual positives, how many were found?
- Important when false negatives are costly
- Example: Medical diagnosis (don't want to miss sick patients)

**F1-Score:**
```
F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
```
- Harmonic mean of precision and recall
- Balances both metrics
- Good for imbalanced datasets

**Specificity:**
```
Specificity = TN / (TN + FP)
```
- True negative rate
- Important in medical testing

### ğŸ¯ Regression Metrics

**Mean Squared Error (MSE):**
```
MSE = (1/n) Ã— âˆ‘(yáµ¢ - Å·áµ¢)Â²
```
- Penalizes large errors more than small ones
- Same units as yÂ²
- Always positive, 0 is perfect

**Root Mean Squared Error (RMSE):**
```
RMSE = âˆšMSE = âˆš[(1/n) Ã— âˆ‘(yáµ¢ - Å·áµ¢)Â²]
```
- Same units as target variable
- Interpretable: average prediction error
- Sensitive to outliers

**Mean Absolute Error (MAE):**
```
MAE = (1/n) Ã— âˆ‘|yáµ¢ - Å·áµ¢|
```
- Average absolute error
- Less sensitive to outliers than RMSE
- Same units as target variable

**RÂ² Score (Coefficient of Determination):**
```
RÂ² = 1 - SS_res/SS_tot

Where:
SS_res = âˆ‘(yáµ¢ - Å·áµ¢)Â²  (residual sum of squares)
SS_tot = âˆ‘(yáµ¢ - È³)Â²   (total sum of squares)
```

**RÂ² Interpretation:**
- RÂ² = 1: Perfect predictions
- RÂ² = 0: Model as good as predicting the mean
- RÂ² < 0: Model worse than predicting the mean
- RÂ² = 0.8: Model explains 80% of variance

---

## ğŸ”§ 5. Cross-Validation Mathematics

### ğŸ¯ K-Fold Cross-Validation

**Process:**
1. Split data into k equal folds
2. For each fold i:
   - Train on k-1 folds
   - Test on fold i
   - Record performance score Sáµ¢

**Final Score:**
```
CV_Score = (1/k) Ã— âˆ‘Sáµ¢
CV_Std = âˆš[(1/k) Ã— âˆ‘(Sáµ¢ - CV_Score)Â²]
```

**Bias-Variance Trade-off:**
- **Small k (e.g., k=3)**: 
  - Higher bias (less data for training)
  - Lower variance (more similar training sets)
  - Faster computation
  
- **Large k (e.g., k=10)**:
  - Lower bias (more data for training)
  - Higher variance (more different training sets)
  - Slower computation

**Leave-One-Out CV (LOOCV):**
- Special case: k = n (number of samples)
- Maximum training data for each fold
- Highest computational cost
- Can have high variance

### ğŸ¯ Stratified Cross-Validation

**For Classification:**
- Maintains class distribution in each fold
- Ensures each fold is representative
- Reduces variance in performance estimates

**Mathematical Constraint:**
```
For each fold f and class c:
P(class=c in fold f) â‰ˆ P(class=c in full dataset)
```

---

## ğŸšï¸ 6. Hyperparameter Tuning Mathematics

### ğŸ¯ Grid Search

**Search Space:**
```
If parameters pâ‚, pâ‚‚, ..., pâ‚– have nâ‚, nâ‚‚, ..., nâ‚– values respectively:
Total combinations = nâ‚ Ã— nâ‚‚ Ã— ... Ã— nâ‚–
```

**With Cross-Validation:**
```
Total model fits = (âˆnáµ¢) Ã— k_folds
```

**Computational Complexity:**
- Exponential in number of parameters
- Exhaustive but guaranteed to find optimum in search space
- Parallelizable across parameter combinations

### ğŸ¯ Random Search

**Probability Theory:**
- For each iteration, randomly sample from parameter distributions
- More efficient than grid search for high dimensions
- Often finds good solutions faster

**Mathematical Justification:**
If optimal parameters lie in top 5% of search space:
- Grid search with 100 points: might miss optimal region
- Random search with 100 points: 99.4% chance to find good solution

**Advantages:**
- Better for continuous parameters
- Scales better with dimensionality
- Can run for any budget

---

*This guide serves as a reference for the mathematical concepts used throughout the ML framework. Each notebook provides practical implementations of these theoretical foundations.*
1. **Classification Problems** - Predicting categories
2. **Decision Trees** - Rule-based learning
3. **Data Visualization** - Understanding patterns
4. **Hyperparameter Tuning** - Optimizing models
5. **Confusion Matrix** - Classification evaluation

---

# ğŸ“– NOTEBOOK 3: CALIFORNIA HOUSING REGRESSION

## Core Concepts You'll Master:
1. **Large Dataset Handling** - Sampling techniques
2. **Advanced Regression** - Decision tree regressors
3. **Grid Search** - Systematic optimization
4. **Model Comparison** - Before/after tuning
5. **Real-world Applications** - Practical ML

---

*This guide will be your complete reference for understanding every aspect of these machine learning implementations.*
