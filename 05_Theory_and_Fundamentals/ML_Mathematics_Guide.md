# ğŸ“š Machine Learning Mathematics Guide
*Complete Mathematical Foundations for ML Framework*

## ğŸ¯ Overview

This guide provides comprehensive mathematical explanations for all algorithms and concepts used in our ML framework. Each section includes the mathematical formulation, intuitive explanation, and practical implementation details.

---

## ğŸ“Š 1. AdaBoost (Adaptive Boosting)

### ğŸ§® Mathematical Foundation

#### AdaBoost Algorithm
AdaBoost combines multiple weak learners sequentially, where each learner focuses on previously misclassified examples.

**Core Algorithm Steps:**

1. **Initialize sample weights:**
```
wâ‚(i) = 1/N  for i = 1, ..., N
```

2. **For each boosting round t = 1, ..., T:**

   a) **Train weak learner hâ‚œ with weighted samples**
   
   b) **Calculate weighted error:**
   ```
   Îµâ‚œ = âˆ‘áµ¢ wâ‚(i) Â· I(hâ‚œ(xáµ¢) â‰  yáµ¢)
   ```
   
   c) **Calculate classifier weight:**
   ```
   Î±â‚› = (1/2) Â· ln((1 - Îµâ‚œ)/Îµâ‚œ)
   ```
   
   d) **Update sample weights:**
   ```
   wâ‚œâ‚Šâ‚(i) = wâ‚œ(i) Â· exp(-Î±â‚› Â· yáµ¢ Â· hâ‚œ(xáµ¢)) / Zâ‚œ
   ```
   where Zâ‚œ is normalization factor

3. **Final classifier:**
```
H(x) = sign(âˆ‘â‚œâ‚Œâ‚áµ€ Î±â‚œ Â· hâ‚œ(x))
```

#### Key Mathematical Insights:

**Weight Update Mechanism:**
- **Correctly classified**: wâ‚œâ‚Šâ‚(i) = wâ‚œ(i) Â· exp(-Î±â‚œ) (weight decreases)
- **Misclassified**: wâ‚œâ‚Šâ‚(i) = wâ‚œ(i) Â· exp(Î±â‚œ) (weight increases)
- **Effect**: Forces next classifier to focus on difficult examples

**Classifier Weight (Î±â‚œ):**
- **Low error (Îµâ‚œ â†’ 0)**: Î±â‚œ â†’ âˆ (high influence)
- **High error (Îµâ‚œ â†’ 0.5)**: Î±â‚œ â†’ 0 (low influence)
- **Random guessing (Îµâ‚œ = 0.5)**: Î±â‚œ = 0 (no contribution)

### ğŸ¯ Key Parameters Explained

#### n_estimators (Number of Weak Learners)
```
Mathematical Impact: H(x) = sign(âˆ‘â‚œâ‚Œâ‚â¿_áµ‰Ë¢áµ—â±áµáµƒáµ—áµ’Ê³Ë¢ Î±â‚œ Â· hâ‚œ(x))
```
- **Small values (10-50)**: Fast training, potential underfitting
- **Large values (100-500)**: Better performance, overfitting risk
- **Trade-off**: Bias vs. Variance, Training time vs. Accuracy

#### learning_rate (Shrinkage Parameter)
```
Modified classifier weight: Î±'â‚œ = learning_rate Ã— Î±â‚œ
Final prediction: H(x) = sign(âˆ‘â‚œâ‚Œâ‚áµ€ Î±'â‚œ Â· hâ‚œ(x))
```
- **High rates (1.0-2.0)**: Aggressive learning, fast convergence
- **Low rates (0.1-0.5)**: Conservative learning, better generalization
- **Mathematical effect**: Scales the contribution of each weak learner

#### algorithm Parameter
**SAMME (Stagewise Additive Modeling using Multi-class Exponential loss):**
```
Uses discrete class predictions: hâ‚œ(x) âˆˆ {-1, +1}
Weight update: standard AdaBoost formula
```

**SAMME.R (Real SAMME):**
```
Uses class probabilities: pâ‚œ(x) âˆˆ [0, 1]
Faster convergence through probability estimates
Better performance in most cases
```

### ğŸ” Loss Function Analysis

#### Exponential Loss (Classification)
```
L(y, f(x)) = exp(-y Â· f(x))
```
- **Correct prediction (yÂ·f(x) > 0)**: Loss < 1
- **Incorrect prediction (yÂ·f(x) < 0)**: Loss > 1
- **Property**: Exponentially penalizes misclassifications

#### Regression Loss Functions
**Linear Loss:**
```
L(y, f(x)) = |y - f(x)|
```

**Square Loss:**
```
L(y, f(x)) = (y - f(x))Â²
```

**Exponential Loss:**
```
L(y, f(x)) = exp(|y - f(x)|)
```

### ğŸ“ Theoretical Properties

#### Generalization Bound
```
P(error) â‰¤ âˆâ‚œâ‚Œâ‚áµ€ 2âˆš(Îµâ‚œ(1 - Îµâ‚œ))
```
- Shows exponential decrease in training error
- Explains AdaBoost's resistance to overfitting
- Connects weak learning to strong learning

#### Margin Theory
```
Margin(x) = (y Â· âˆ‘â‚œ Î±â‚œhâ‚œ(x)) / âˆ‘â‚œ Î±â‚œ
```
- AdaBoost tends to maximize margins
- Larger margins â†’ better generalization
- Explains continued improvement after zero training error

## ğŸ“Š 2. Gradient Boosting

### ğŸ§® Mathematical Foundation

#### Gradient Boosting Algorithm
Gradient Boosting builds an additive model by sequentially adding weak learners that minimize a loss function using gradient descent in function space.

**Core Algorithm:**

1. **Initialize model with constant:**
```
Fâ‚€(x) = argmin_Î³ âˆ‘áµ¢ L(yáµ¢, Î³)
```

2. **For each boosting iteration m = 1, ..., M:**

   a) **Compute negative gradient (pseudo-residuals):**
   ```
   ráµ¢â‚˜ = -[âˆ‚L(yáµ¢, F(xáµ¢))/âˆ‚F(xáµ¢)]_{F=F_{m-1}}
   ```
   
   b) **Fit weak learner to pseudo-residuals:**
   ```
   hâ‚˜(x) = argmin_h âˆ‘áµ¢ (ráµ¢â‚˜ - h(xáµ¢))Â²
   ```
   
   c) **Find optimal step size:**
   ```
   Î³â‚˜ = argmin_Î³ âˆ‘áµ¢ L(yáµ¢, F_{m-1}(xáµ¢) + Î³hâ‚˜(xáµ¢))
   ```
   
   d) **Update model:**
   ```
   Fâ‚˜(x) = F_{m-1}(x) + Î³â‚˜hâ‚˜(x)
   ```

3. **Final model:**
```
F(x) = Fâ‚€(x) + âˆ‘â‚˜â‚Œâ‚á´¹ Î³â‚˜hâ‚˜(x)
```

#### Key Mathematical Insights:

**Functional Gradient Descent:**
- **Function space optimization**: Unlike parameter space, optimizes entire function
- **Steepest descent direction**: Negative gradient points to fastest loss decrease
- **Sequential correction**: Each model corrects errors of previous ensemble

**Loss Function Gradients:**

**Squared Loss (Regression):**
```
L(y, F(x)) = (y - F(x))Â²/2
âˆ‚L/âˆ‚F = -(y - F(x)) = -residual
```

**Deviance Loss (Classification):**
```
L(y, F(x)) = log(1 + exp(-2yF(x)))
âˆ‚L/âˆ‚F = -2y/(1 + exp(2yF(x)))
```

**Huber Loss (Robust Regression):**
```
L(y, F(x)) = {
  (y - F(x))Â²/2           if |y - F(x)| â‰¤ Î´
  Î´|y - F(x)| - Î´Â²/2      otherwise
}
```

### ğŸ¯ Key Parameters Mathematical Analysis

#### learning_rate (Shrinkage Parameter)
```
Fâ‚˜(x) = F_{m-1}(x) + Î½ Â· Î³â‚˜hâ‚˜(x)  where Î½ = learning_rate
```
- **Mathematical effect**: Scales step size in function space
- **Bias-variance trade-off**: Lower Î½ reduces variance, increases bias
- **Regularization**: Prevents overfitting through smaller updates

#### n_estimators (Number of Boosting Stages)
```
Final model: F(x) = Fâ‚€(x) + âˆ‘â‚˜â‚Œâ‚â¿_áµ‰Ë¢áµ—â±áµáµƒáµ—áµ’Ê³Ë¢ Î½Î³â‚˜hâ‚˜(x)
```
- **Capacity control**: More stages increase model complexity
- **Early stopping**: Optimal number prevents overfitting
- **Training error**: Monotonically decreases with more stages

#### subsample (Stochastic Gradient Boosting)
```
Each hâ‚˜ trained on random subset of size: subsample Ã— N
```
- **Variance reduction**: Random sampling reduces overfitting
- **Computational efficiency**: Faster training on smaller subsets
- **Regularization effect**: Introduces beneficial noise

#### max_depth (Tree Complexity)
```
Interaction level â‰ˆ max_depth
Number of regions â‰¤ 2^max_depth
```
- **Feature interactions**: Deeper trees capture higher-order interactions
- **Model complexity**: Exponential growth in number of leaf nodes
- **Bias-variance**: Deeper trees reduce bias, increase variance

### ğŸ” Loss Functions Deep Analysis

#### Classification Losses
**Deviance (Logistic Loss):**
```
L(y, F(x)) = log(1 + exp(-yF(x)))
```
- **Properties**: Smooth, differentiable, convex
- **Probabilistic**: Connects to logistic regression
- **Robust**: Less sensitive to outliers than exponential loss

**Exponential Loss:**
```
L(y, F(x)) = exp(-yF(x))
```
- **Connection**: Equivalent to AdaBoost algorithm
- **Sensitivity**: Very sensitive to outliers
- **Convergence**: Fast convergence properties

#### Regression Losses
**Squared Error:**
```
L(y, F(x)) = (y - F(x))Â²/2
```
- **Gradient**: -(y - F(x)) = negative residual
- **Properties**: Smooth, easy optimization
- **Outlier sensitivity**: High sensitivity to outliers

**Absolute Error (LAD):**
```
L(y, F(x)) = |y - F(x)|
```
- **Robustness**: Robust to outliers
- **Median regression**: Estimates conditional median
- **Non-smooth**: Requires special handling at zero

**Huber Loss:**
```
L(y, F(x)) = {
  (y - F(x))Â²/2           if |y - F(x)| â‰¤ Î´
  Î´|y - F(x)| - Î´Â²/2      otherwise
}
```
- **Compromise**: Combines squared and absolute loss benefits
- **Parameter Î´**: Controls transition point
- **Robustness**: Less sensitive to outliers than squared loss

### ğŸ“ Theoretical Properties

#### Convergence Guarantees
For appropriate learning rates and sufficient trees:
```
lim_{Mâ†’âˆ} F_M(x) = F*(x) = argmin_F E[L(y, F(x))]
```

#### Generalization Bound
```
E[L(y, F_M(x))] â‰¤ E[L(y, F*(x))] + O(1/âˆšN) + complexity_penalty
```

#### Feature Importance
```
Importance_j = âˆ‘_{m=1}^M âˆ‘_{internal_nodes} IÂ²_m(v) Â· I(v splits on feature j)
```
where IÂ²_m(v) is squared improvement from split at node v.

## ğŸ“Š 3. Support Vector Machines (SVM)

### ğŸ§® Mathematical Foundation

#### Linear SVM Optimization Problem
```
Minimize: (1/2)||w||Â² + Câˆ‘Î¾áµ¢

Subject to:
- yáµ¢(wÂ·xáµ¢ + b) â‰¥ 1 - Î¾áµ¢  for all i
- Î¾áµ¢ â‰¥ 0  for all i
```

**Where:**
- `w`: weight vector (defines the separating hyperplane)
- `b`: bias term (shifts the hyperplane)
- `C`: regularization parameter (controls trade-off)
- `Î¾áµ¢`: slack variables (allow for misclassification)
- `xáµ¢, yáµ¢`: training samples and labels

#### What Each Component Does:

**Objective Function: (1/2)||w||Â²**
- **Purpose**: Maximize margin between classes
- **Math**: ||w||Â² = wâ‚Â² + wâ‚‚Â² + ... + wâ‚™Â²
- **Intuition**: Smaller weights â†’ larger margin â†’ better generalization
- **Why 1/2**: Mathematical convenience for derivatives

**Regularization Term: Câˆ‘Î¾áµ¢**
- **Purpose**: Control tolerance for misclassification
- **High C**: Low tolerance (hard margin, might overfit)
- **Low C**: High tolerance (soft margin, might underfit)
- **Î¾áµ¢ = 0**: Point is correctly classified with sufficient margin
- **0 < Î¾áµ¢ < 1**: Point is correctly classified but within margin
- **Î¾áµ¢ â‰¥ 1**: Point is misclassified

### ğŸ¯ Kernel Functions

#### RBF (Radial Basis Function) Kernel
```
K(x, x') = exp(-Î³||x - x'||Â²)
```

**Mathematical Breakdown:**
- `||x - x'||Â²`: Squared Euclidean distance between points
- `Î³`: Kernel parameter (gamma)
  - High Î³: Tight, complex decision boundaries
  - Low Î³: Smooth, simple decision boundaries
- `exp()`: Exponential function creates smooth transitions

**How RBF Works:**
1. Calculate distance between every pair of points
2. Apply exponential decay: closer points â†’ higher similarity
3. Create infinite-dimensional feature space implicitly
4. Enable non-linear decision boundaries in original space

#### Polynomial Kernel
```
K(x, x') = (Î³âŸ¨x, x'âŸ© + r)áµˆ
```

**Parameters:**
- `d`: degree of polynomial (typically 2-4)
- `Î³`: scaling parameter
- `r`: independent term
- `âŸ¨x, x'âŸ©`: dot product of feature vectors

**Example (degree=2):**
- Original features: (xâ‚, xâ‚‚)
- Expanded features: (xâ‚Â², xâ‚‚Â², âˆš2xâ‚xâ‚‚, âˆš2xâ‚, âˆš2xâ‚‚, 1)
- Creates quadratic decision boundaries

### ğŸ”¢ SVM Decision Function
```
f(x) = sign(âˆ‘Î±áµ¢yáµ¢K(xáµ¢, x) + b)
```

**Components:**
- `Î±áµ¢`: Lagrange multipliers (learned during training)
- `yáµ¢`: class labels of support vectors
- `K(xáµ¢, x)`: kernel function
- `b`: bias term
- `sign()`: outputs +1 or -1 for classification

**Support Vectors:**
- Training points with Î±áµ¢ > 0
- Points that define the decision boundary
- Typically only 5-20% of training data
- Removing non-support vectors doesn't change the model

---

## ï¿½ 2. Naive Bayes

### ğŸ§® Mathematical Foundation

#### Bayes' Theorem
```
P(y|x) = P(x|y) Ã— P(y) / P(x)
```

**For Classification:**
```
P(class|features) = P(features|class) Ã— P(class) / P(features)
```

**Naive Independence Assumption:**
```
P(xâ‚, xâ‚‚, ..., xâ‚™|y) = P(xâ‚|y) Ã— P(xâ‚‚|y) Ã— ... Ã— P(xâ‚™|y)
```

### ğŸ¯ Gaussian Naive Bayes

#### Probability Density Function
```
P(xáµ¢|y) = (1/âˆš(2Ï€Ïƒáµ§Â²)) Ã— exp(-(xáµ¢ - Î¼áµ§)Â²/(2Ïƒáµ§Â²))
```

**Parameters (learned from training data):**
- `Î¼áµ§`: mean of feature xáµ¢ for class y
- `Ïƒáµ§Â²`: variance of feature xáµ¢ for class y

**Step-by-Step Calculation:**

1. **Learn Parameters (Training):**
   ```
   Î¼áµ§ = (1/náµ§) Ã— âˆ‘(xáµ¢ where yáµ¢ = y)
   Ïƒáµ§Â² = (1/náµ§) Ã— âˆ‘(xáµ¢ - Î¼áµ§)Â² where yáµ¢ = y
   ```

2. **Calculate Class Probabilities (Prediction):**
   ```
   P(y) = count(y) / total_samples
   ```

3. **Calculate Feature Likelihoods:**
   ```
   For each feature xáµ¢:
   P(xáµ¢|y) = gaussian_pdf(xáµ¢, Î¼áµ§, Ïƒáµ§)
   ```

4. **Combine Using Naive Assumption:**
   ```
   P(y|x) âˆ P(y) Ã— âˆP(xáµ¢|y)
   ```

**Why "Naive":**
- Assumes features are independent given the class
- Often violated in practice, but works surprisingly well
- Example: In text classification, word occurrences are not independent
- Despite violation, often achieves good performance

### ğŸ”¢ Prediction Formula
```
Å· = argmax P(y) Ã— âˆP(xáµ¢|y)
     y
```

**Practical Implementation (log probabilities):**
```
log P(y|x) = log P(y) + âˆ‘log P(xáµ¢|y)
```

**Why use log probabilities:**
- Avoids numerical underflow (multiplying many small numbers)
- Addition is faster than multiplication
- Monotonic transformation preserves ordering

---

## ğŸŒ³ 3. Ensemble Methods

### ğŸ¯ Random Forest Mathematics

#### Bagging (Bootstrap Aggregating)

**Bootstrap Sampling:**
- Create B bootstrap samples from original dataset
- Each sample: randomly select n samples with replacement
- Some samples appear multiple times, others not at all
- Reduces variance by averaging multiple models

**Random Feature Selection:**
- At each node split, randomly select m features from total p features
- Typical values: m = âˆšp for classification, m = p/3 for regression
- Reduces correlation between trees
- Improves generalization

#### Prediction Aggregation

**Classification (Majority Voting):**
```
Å· = mode{hâ‚(x), hâ‚‚(x), ..., hB(x)}
```

**Regression (Average):**
```
Å· = (1/B) Ã— âˆ‘háµ¦(x)
        b=1
```

**Out-of-Bag (OOB) Error:**
```
OOB Error = (1/n) Ã— âˆ‘I(yáµ¢ â‰  Å·áµ¢^OOB)
```
- Uses samples not included in bootstrap for validation
- No need for separate validation set
- Unbiased estimate of generalization error

### ğŸ¯ AdaBoost Mathematics

#### Adaptive Boosting Algorithm

**Weight Update Rule:**
```
wáµ¢^(t+1) = wáµ¢^(t) Ã— exp(-Î±â‚œyáµ¢hâ‚œ(xáµ¢)) / Zâ‚œ
```

**Where:**
- `wáµ¢^(t)`: weight of sample i at iteration t
- `Î±â‚œ`: weight of classifier t
- `hâ‚œ(xáµ¢)`: prediction of classifier t on sample i
- `Zâ‚œ`: normalization constant

**Classifier Weight:**
```
Î±â‚œ = (1/2) Ã— ln((1 - Îµâ‚œ)/Îµâ‚œ)
```

**Where:**
- `Îµâ‚œ`: weighted error rate of classifier t
- Higher accuracy â†’ higher weight in final prediction

**Final Prediction:**
```
H(x) = sign(âˆ‘Î±â‚œhâ‚œ(x))
        t=1
```

**Key Insights:**
- Misclassified samples get higher weights
- Next classifier focuses on difficult samples
- Sequentially reduces training error
- Can overfit if not regularized

---

## ğŸ“ 4. Performance Metrics

### ğŸ¯ Classification Metrics

#### Confusion Matrix Components
```
                Prediction
Actual    |  Pos  |  Neg  |
----------|-------|-------|
Positive  |  TP   |  FN   |
Negative  |  FP   |  TN   |
```

#### Metric Formulas

**Accuracy:**
```
Accuracy = (TP + TN) / (TP + TN + FP + FN)
```
- Overall correctness
- Good for balanced datasets
- Misleading for imbalanced datasets

**Precision:**
```
Precision = TP / (TP + FP)
```
- Of positive predictions, how many were correct?
- Important when false positives are costly
- Example: Spam detection (don't want to mark important emails as spam)

**Recall (Sensitivity):**
```
Recall = TP / (TP + FN)
```
- Of actual positives, how many were found?
- Important when false negatives are costly
- Example: Medical diagnosis (don't want to miss sick patients)

**F1-Score:**
```
F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
```
- Harmonic mean of precision and recall
- Balances both metrics
- Good for imbalanced datasets

**Specificity:**
```
Specificity = TN / (TN + FP)
```
- True negative rate
- Important in medical testing

### ğŸ¯ Regression Metrics

**Mean Squared Error (MSE):**
```
MSE = (1/n) Ã— âˆ‘(yáµ¢ - Å·áµ¢)Â²
```
- Penalizes large errors more than small ones
- Same units as yÂ²
- Always positive, 0 is perfect

**Root Mean Squared Error (RMSE):**
```
RMSE = âˆšMSE = âˆš[(1/n) Ã— âˆ‘(yáµ¢ - Å·áµ¢)Â²]
```
- Same units as target variable
- Interpretable: average prediction error
- Sensitive to outliers

**Mean Absolute Error (MAE):**
```
MAE = (1/n) Ã— âˆ‘|yáµ¢ - Å·áµ¢|
```
- Average absolute error
- Less sensitive to outliers than RMSE
- Same units as target variable

**RÂ² Score (Coefficient of Determination):**
```
RÂ² = 1 - SS_res/SS_tot

Where:
SS_res = âˆ‘(yáµ¢ - Å·áµ¢)Â²  (residual sum of squares)
SS_tot = âˆ‘(yáµ¢ - È³)Â²   (total sum of squares)
```

**RÂ² Interpretation:**
- RÂ² = 1: Perfect predictions
- RÂ² = 0: Model as good as predicting the mean
- RÂ² < 0: Model worse than predicting the mean
- RÂ² = 0.8: Model explains 80% of variance

---

## ğŸ”§ 5. Cross-Validation Mathematics

### ğŸ¯ K-Fold Cross-Validation

**Process:**
1. Split data into k equal folds
2. For each fold i:
   - Train on k-1 folds
   - Test on fold i
   - Record performance score Sáµ¢

**Final Score:**
```
CV_Score = (1/k) Ã— âˆ‘Sáµ¢
CV_Std = âˆš[(1/k) Ã— âˆ‘(Sáµ¢ - CV_Score)Â²]
```

**Bias-Variance Trade-off:**
- **Small k (e.g., k=3)**: 
  - Higher bias (less data for training)
  - Lower variance (more similar training sets)
  - Faster computation
  
- **Large k (e.g., k=10)**:
  - Lower bias (more data for training)
  - Higher variance (more different training sets)
  - Slower computation

**Leave-One-Out CV (LOOCV):**
- Special case: k = n (number of samples)
- Maximum training data for each fold
- Highest computational cost
- Can have high variance

### ğŸ¯ Stratified Cross-Validation

**For Classification:**
- Maintains class distribution in each fold
- Ensures each fold is representative
- Reduces variance in performance estimates

**Mathematical Constraint:**
```
For each fold f and class c:
P(class=c in fold f) â‰ˆ P(class=c in full dataset)
```

---

## ï¿½ 6. K-Nearest Neighbors (KNN)

### ğŸ§® Mathematical Foundation

#### Distance Metrics

**Minkowski Distance (General Form):**
```
d(x, y) = (âˆ‘áµ¢â‚Œâ‚â¿ |xáµ¢ - yáµ¢|áµ–)^(1/p)
```

**Special Cases:**
- **Manhattan Distance (p=1):** `d(x,y) = âˆ‘|xáµ¢ - yáµ¢|`
- **Euclidean Distance (p=2):** `d(x,y) = âˆš(âˆ‘(xáµ¢ - yáµ¢)Â²)`
- **Chebyshev Distance (p=âˆ):** `d(x,y) = max|xáµ¢ - yáµ¢|`

#### KNN Classification

**Uniform Weighting:**
```
Å· = argmax_c âˆ‘áµ¢âˆˆNâ‚–(x) I(yáµ¢ = c)
```
where Nâ‚–(x) are the k nearest neighbors of x

**Distance Weighting:**
```
Å· = argmax_c âˆ‘áµ¢âˆˆNâ‚–(x) wáµ¢ Â· I(yáµ¢ = c)
```
where `wáµ¢ = 1/(d(x, xáµ¢) + Îµ)` and Îµ prevents division by zero

#### KNN Regression

**Uniform Weighting:**
```
Å· = (1/k) âˆ‘áµ¢âˆˆNâ‚–(x) yáµ¢
```

**Distance Weighting:**
```
Å· = (âˆ‘áµ¢âˆˆNâ‚–(x) wáµ¢ Â· yáµ¢) / (âˆ‘áµ¢âˆˆNâ‚–(x) wáµ¢)
```

#### Key Mathematical Properties:

**1. Non-parametric Nature:**
- No explicit model parameters to learn
- Decision boundary complexity increases with data

**2. Lazy Learning:**
- No training phase: O(1) training time
- All computation at prediction: O(N) query time

**3. Voronoi Tessellation:**
- KNN creates implicit Voronoi regions
- Decision boundaries are piecewise linear

**4. Curse of Dimensionality:**
- In high dimensions: `||x-y||â‚‚ â†’ constant` for all pairs
- Distance concentration: `max d / min d â†’ 1` as d â†’ âˆ

#### Computational Complexity

**Brute Force Search:**
- Time: O(NÂ·d) per query
- Space: O(NÂ·d) for storage

**Tree-based Methods (KD-tree, Ball-tree):**
- Construction: O(N log N)
- Query: O(log N) in low dimensions, degrades to O(N) in high dimensions
- Ball-tree performs better in high dimensions

#### Bias-Variance Trade-off

**Small k (k=1,3,5):**
- Low bias: captures local patterns
- High variance: sensitive to noise
- Overfitting tendency

**Large k (kâ†’N):**
- High bias: overly smooth predictions
- Low variance: stable predictions
- Underfitting tendency

**Optimal k Selection:**
```
k* = argmin_k E[(f(x) - fÌ‚â‚–(x))Â²]
```
Often approximated by cross-validation

---

## ï¿½ğŸšï¸ 7. Hyperparameter Tuning Mathematics

### ğŸ¯ Grid Search

**Search Space:**
```
If parameters pâ‚, pâ‚‚, ..., pâ‚– have nâ‚, nâ‚‚, ..., nâ‚– values respectively:
Total combinations = nâ‚ Ã— nâ‚‚ Ã— ... Ã— nâ‚–
```

**With Cross-Validation:**
```
Total model fits = (âˆnáµ¢) Ã— k_folds
```

**Computational Complexity:**
- Exponential in number of parameters
- Exhaustive but guaranteed to find optimum in search space
- Parallelizable across parameter combinations

### ğŸ¯ Random Search

**Probability Theory:**
- For each iteration, randomly sample from parameter distributions
- More efficient than grid search for high dimensions
- Often finds good solutions faster

**Mathematical Justification:**
If optimal parameters lie in top 5% of search space:
- Grid search with 100 points: might miss optimal region
- Random search with 100 points: 99.4% chance to find good solution

**Advantages:**
- Better for continuous parameters
- Scales better with dimensionality
- Can run for any budget

---

*This guide serves as a reference for the mathematical concepts used throughout the ML framework. Each notebook provides practical implementations of these theoretical foundations.*
1. **Classification Problems** - Predicting categories
2. **Decision Trees** - Rule-based learning
3. **Data Visualization** - Understanding patterns
4. **Hyperparameter Tuning** - Optimizing models
5. **Confusion Matrix** - Classification evaluation

---

# ğŸ“– NOTEBOOK 3: CALIFORNIA HOUSING REGRESSION

## Core Concepts You'll Master:
1. **Large Dataset Handling** - Sampling techniques
2. **Advanced Regression** - Decision tree regressors
3. **Grid Search** - Systematic optimization
4. **Model Comparison** - Before/after tuning
5. **Real-world Applications** - Practical ML

---

*This guide will be your complete reference for understanding every aspect of these machine learning implementations.*
