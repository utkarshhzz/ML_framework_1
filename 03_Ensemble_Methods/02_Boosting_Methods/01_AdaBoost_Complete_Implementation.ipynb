{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93b6b1ec",
   "metadata": {},
   "source": [
    "# AdaBoost (Adaptive Boosting) Complete Implementation Guide\n",
    "\n",
    "## Overview\n",
    "AdaBoost is a powerful ensemble learning algorithm that combines multiple weak learners (typically decision stumps) to create a strong classifier. This notebook provides comprehensive coverage of AdaBoost for both classification and regression tasks, with detailed hyperparameter optimization using GridSearchCV.\n",
    "\n",
    "## Learning Objectives\n",
    "- Master AdaBoost algorithm principles and implementation\n",
    "- Understand comprehensive hyperparameter tuning with GridSearchCV\n",
    "- Compare baseline vs optimized model performance\n",
    "- Analyze parameter impact on model performance\n",
    "- Implement complete prediction pipelines for deployment\n",
    "\n",
    "## Key Concepts\n",
    "- **Adaptive Boosting**: Sequential learning focusing on misclassified examples\n",
    "- **Weak Learners**: Simple models (decision stumps) combined into strong ensemble\n",
    "- **Weight Adjustment**: Dynamic sample weight updates based on prediction errors\n",
    "- **GridSearchCV**: Systematic hyperparameter optimization with cross-validation\n",
    "\n",
    "## Technical Stack\n",
    "- **scikit-learn**: AdaBoost implementation and optimization tools\n",
    "- **GridSearchCV**: Comprehensive hyperparameter search\n",
    "- **Performance Analysis**: Multiple evaluation metrics and visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881df8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Essential Libraries for Comprehensive AdaBoost Implementation\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, validation_curve\n",
    "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor\n",
    "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix,\n",
    "                           mean_squared_error, mean_absolute_error, r2_score)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style for professional plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")\n",
    "print(\"üìä Visualization style configured\")\n",
    "print(\"üöÄ Ready for comprehensive AdaBoost implementation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f457cc9",
   "metadata": {},
   "source": [
    "## 1. Dataset Creation and Exploration\n",
    "\n",
    "We'll create both classification and regression datasets to demonstrate AdaBoost's versatility across different problem types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0149d390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Classification Dataset\n",
    "X_clf, y_clf = make_classification(\n",
    "    n_samples=1000,          # Total samples for robust training/testing\n",
    "    n_features=20,           # Feature dimensionality\n",
    "    n_informative=15,        # Informative features for classification\n",
    "    n_redundant=3,           # Redundant features (linear combinations)\n",
    "    n_clusters_per_class=1,  # Cluster structure per class\n",
    "    random_state=42          # Reproducible results\n",
    ")\n",
    "\n",
    "# Create Regression Dataset\n",
    "X_reg, y_reg = make_regression(\n",
    "    n_samples=1000,          # Total samples\n",
    "    n_features=15,           # Feature dimensionality\n",
    "    n_informative=12,        # Informative features\n",
    "    noise=0.1,               # Noise level\n",
    "    random_state=42          # Reproducible results\n",
    ")\n",
    "\n",
    "# Split datasets\n",
    "X_clf_train, X_clf_test, y_clf_train, y_clf_test = train_test_split(\n",
    "    X_clf, y_clf, test_size=0.2, random_state=42, stratify=y_clf\n",
    ")\n",
    "\n",
    "X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"üìä Dataset Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Classification Dataset:\")\n",
    "print(f\"  ‚Ä¢ Training samples: {X_clf_train.shape[0]}\")\n",
    "print(f\"  ‚Ä¢ Testing samples: {X_clf_test.shape[0]}\")\n",
    "print(f\"  ‚Ä¢ Features: {X_clf_train.shape[1]}\")\n",
    "print(f\"  ‚Ä¢ Class distribution: {dict(zip(*np.unique(y_clf, return_counts=True)))}\")\n",
    "\n",
    "print(f\"\\nRegression Dataset:\")\n",
    "print(f\"  ‚Ä¢ Training samples: {X_reg_train.shape[0]}\")\n",
    "print(f\"  ‚Ä¢ Testing samples: {X_reg_test.shape[0]}\")\n",
    "print(f\"  ‚Ä¢ Features: {X_reg_train.shape[1]}\")\n",
    "print(f\"  ‚Ä¢ Target range: [{y_reg.min():.2f}, {y_reg.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eef722c",
   "metadata": {},
   "source": [
    "## 2. GridSearchCV Setup and Configuration\n",
    "\n",
    "GridSearchCV provides systematic hyperparameter optimization through exhaustive search over parameter combinations with cross-validation. Understanding its configuration is crucial for effective model tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba088f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure GridSearchCV Parameters for Comprehensive Search\n",
    "\n",
    "def create_gridsearch_config(estimator, param_grid, task_type='classification'):\n",
    "    \"\"\"\n",
    "    Create standardized GridSearchCV configuration with optimal settings.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    estimator : sklearn estimator\n",
    "        The base estimator to optimize\n",
    "    param_grid : dict\n",
    "        Parameter grid for systematic search\n",
    "    task_type : str\n",
    "        'classification' or 'regression' for appropriate scoring\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    GridSearchCV object with optimized configuration\n",
    "    \"\"\"\n",
    "    \n",
    "    # Select appropriate scoring metric\n",
    "    scoring = 'accuracy' if task_type == 'classification' else 'neg_mean_squared_error'\n",
    "    \n",
    "    # Configure GridSearchCV with comprehensive settings\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=estimator,        # Base model to optimize\n",
    "        param_grid=param_grid,      # Parameter combinations to test\n",
    "        cv=5,                       # 5-fold cross-validation for robust evaluation\n",
    "        scoring=scoring,            # Optimization metric\n",
    "        n_jobs=-1,                  # Use all available CPU cores\n",
    "        verbose=3,                  # Detailed progress reporting\n",
    "        return_train_score=True,    # Return training scores for analysis\n",
    "        refit=True                  # Refit best model on full training set\n",
    "    )\n",
    "    \n",
    "    return grid_search\n",
    "\n",
    "print(\"‚öôÔ∏è GridSearchCV Configuration Function Created\")\n",
    "print(\"üîß Key Configuration Parameters:\")\n",
    "print(\"   ‚Ä¢ Cross-validation: 5-fold for robust performance estimation\")\n",
    "print(\"   ‚Ä¢ Parallel processing: All available CPU cores (-1)\")\n",
    "print(\"   ‚Ä¢ Verbose output: Detailed progress monitoring (level 3)\")\n",
    "print(\"   ‚Ä¢ Training scores: Included for overfitting analysis\")\n",
    "print(\"   ‚Ä¢ Automatic refit: Best model fitted on complete training data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f330612",
   "metadata": {},
   "source": [
    "## 3. Parameter Grid Definition and Detailed Explanation\n",
    "\n",
    "Understanding each AdaBoost parameter's impact is crucial for effective hyperparameter tuning. Let's define comprehensive parameter grids with detailed explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fad477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Comprehensive Parameter Grids with Detailed Explanations\n",
    "\n",
    "# AdaBoost Classification Parameter Grid\n",
    "param_grid_classification = {\n",
    "    'n_estimators': [50, 100, 200],           # Number of boosting stages\n",
    "    'learning_rate': [0.01, 0.1, 1.0, 1.5, 2.0],  # Shrinkage parameter\n",
    "    'algorithm': ['SAMME', 'SAMME.R']         # Boosting algorithm variant\n",
    "}\n",
    "\n",
    "# AdaBoost Regression Parameter Grid  \n",
    "param_grid_regression = {\n",
    "    'n_estimators': [50, 100, 200],           # Number of boosting stages\n",
    "    'learning_rate': [0.01, 0.1, 1.0, 1.5, 2.0],  # Shrinkage parameter\n",
    "    'loss': ['linear', 'square', 'exponential']     # Loss function for regression\n",
    "}\n",
    "\n",
    "print(\"üéØ AdaBoost Parameter Grid Analysis\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nüìã CLASSIFICATION PARAMETERS:\")\n",
    "print(\"‚îÅ\" * 40)\n",
    "\n",
    "print(\"\\nüî¢ n_estimators (Number of Boosting Stages):\")\n",
    "print(\"   ‚Ä¢ 50: Fast training, may underfit\")\n",
    "print(\"   ‚Ä¢ 100: Balanced performance/speed\")\n",
    "print(\"   ‚Ä¢ 200: Better accuracy, slower training\")\n",
    "print(\"   üí° Impact: More estimators = better performance but higher overfitting risk\")\n",
    "\n",
    "print(\"\\nüìà learning_rate (Shrinkage Parameter):\")\n",
    "print(\"   ‚Ä¢ 0.01: Very conservative, requires many estimators\")\n",
    "print(\"   ‚Ä¢ 0.1: Conservative, good generalization\")  \n",
    "print(\"   ‚Ä¢ 1.0: Default, no shrinkage\")\n",
    "print(\"   ‚Ä¢ 1.5: Aggressive learning\")\n",
    "print(\"   ‚Ä¢ 2.0: Very aggressive, high overfitting risk\")\n",
    "print(\"   üí° Impact: Lower rates = better generalization, higher rates = faster convergence\")\n",
    "\n",
    "print(\"\\nüîÑ algorithm (Boosting Algorithm Variant):\")\n",
    "print(\"   ‚Ä¢ SAMME: Discrete AdaBoost, uses class predictions\")\n",
    "print(\"   ‚Ä¢ SAMME.R: Real AdaBoost, uses class probabilities (default)\")\n",
    "print(\"   üí° Impact: SAMME.R typically faster and more accurate\")\n",
    "\n",
    "print(\"\\nüìã REGRESSION PARAMETERS:\")\n",
    "print(\"‚îÅ\" * 40)\n",
    "\n",
    "print(\"\\nüìâ loss (Loss Function for Regression):\")\n",
    "print(\"   ‚Ä¢ linear: Linear loss, less sensitive to outliers\")\n",
    "print(\"   ‚Ä¢ square: Squared loss, standard for regression\")\n",
    "print(\"   ‚Ä¢ exponential: Exponential loss, very sensitive to outliers\")\n",
    "print(\"   üí° Impact: Choice affects sensitivity to outliers and convergence behavior\")\n",
    "\n",
    "# Calculate total parameter combinations\n",
    "clf_combinations = np.prod([len(values) for values in param_grid_classification.values()])\n",
    "reg_combinations = np.prod([len(values) for values in param_grid_regression.values()])\n",
    "\n",
    "print(f\"\\nüîç Search Space Analysis:\")\n",
    "print(f\"   ‚Ä¢ Classification combinations: {clf_combinations}\")\n",
    "print(f\"   ‚Ä¢ Regression combinations: {reg_combinations}\")\n",
    "print(f\"   ‚Ä¢ Total evaluations with 5-fold CV: {(clf_combinations + reg_combinations) * 5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28211beb",
   "metadata": {},
   "source": [
    "## 4. Comprehensive Hyperparameter Tuning Execution\n",
    "\n",
    "Now we'll execute the systematic hyperparameter search for both classification and regression tasks, monitoring the optimization progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066846b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Comprehensive AdaBoost Classification Optimization\n",
    "\n",
    "print(\"üöÄ Starting AdaBoost Classification Hyperparameter Optimization\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "# Initialize base AdaBoost classifier\n",
    "ada_classifier = AdaBoostClassifier(random_state=42)\n",
    "\n",
    "# Create GridSearchCV for classification\n",
    "grid_search_clf = create_gridsearch_config(\n",
    "    estimator=ada_classifier,\n",
    "    param_grid=param_grid_classification,\n",
    "    task_type='classification'\n",
    ")\n",
    "\n",
    "print(\"‚è≥ Executing comprehensive parameter search...\")\n",
    "print(\"   This process evaluates all parameter combinations with 5-fold CV\")\n",
    "print(\"   Progress will be displayed below...\\n\")\n",
    "\n",
    "# Execute the grid search\n",
    "grid_search_clf.fit(X_clf_train, y_clf_train)\n",
    "\n",
    "print(f\"\\n‚úÖ Classification Optimization Complete!\")\n",
    "print(f\"üèÜ Best Parameters: {grid_search_clf.best_params_}\")\n",
    "print(f\"üìä Best CV Accuracy: {grid_search_clf.best_score_:.4f}\")\n",
    "print(f\"‚ö° Total fits performed: {len(grid_search_clf.cv_results_)}\")\n",
    "\n",
    "# Store classification results for later analysis\n",
    "best_clf_model = grid_search_clf.best_estimator_\n",
    "best_clf_params = grid_search_clf.best_params_\n",
    "best_clf_score = grid_search_clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b955ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Comprehensive AdaBoost Regression Optimization\n",
    "\n",
    "print(\"\\nüöÄ Starting AdaBoost Regression Hyperparameter Optimization\")\n",
    "print(\"=\" * 62)\n",
    "\n",
    "# Initialize base AdaBoost regressor\n",
    "ada_regressor = AdaBoostRegressor(random_state=42)\n",
    "\n",
    "# Create GridSearchCV for regression\n",
    "grid_search_reg = create_gridsearch_config(\n",
    "    estimator=ada_regressor,\n",
    "    param_grid=param_grid_regression,\n",
    "    task_type='regression'\n",
    ")\n",
    "\n",
    "print(\"‚è≥ Executing comprehensive parameter search...\")\n",
    "print(\"   Optimizing for negative mean squared error\")\n",
    "print(\"   Progress will be displayed below...\\n\")\n",
    "\n",
    "# Execute the grid search\n",
    "grid_search_reg.fit(X_reg_train, y_reg_train)\n",
    "\n",
    "print(f\"\\n‚úÖ Regression Optimization Complete!\")\n",
    "print(f\"üèÜ Best Parameters: {grid_search_reg.best_params_}\")\n",
    "print(f\"üìä Best CV Score (neg_MSE): {grid_search_reg.best_score_:.4f}\")\n",
    "print(f\"üìà Best CV RMSE: {np.sqrt(-grid_search_reg.best_score_):.4f}\")\n",
    "print(f\"‚ö° Total fits performed: {len(grid_search_reg.cv_results_)}\")\n",
    "\n",
    "# Store regression results for later analysis  \n",
    "best_reg_model = grid_search_reg.best_estimator_\n",
    "best_reg_params = grid_search_reg.best_params_\n",
    "best_reg_score = grid_search_reg.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f86c99e",
   "metadata": {},
   "source": [
    "## 5. Best Model Extraction and Validation\n",
    "\n",
    "With optimization complete, let's extract the best models and validate their performance on test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d351de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate Best Classification Model Performance\n",
    "\n",
    "print(\"üéØ Best Classification Model Validation\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Make predictions with optimized classifier\n",
    "y_clf_pred_optimized = best_clf_model.predict(X_clf_test)\n",
    "clf_test_accuracy = accuracy_score(y_clf_test, y_clf_pred_optimized)\n",
    "\n",
    "# Create baseline for comparison\n",
    "baseline_clf = AdaBoostClassifier(random_state=42)\n",
    "baseline_clf.fit(X_clf_train, y_clf_train)\n",
    "y_clf_pred_baseline = baseline_clf.predict(X_clf_test)\n",
    "clf_baseline_accuracy = accuracy_score(y_clf_test, y_clf_pred_baseline)\n",
    "\n",
    "print(f\"üìä Classification Performance Summary:\")\n",
    "print(f\"   ‚Ä¢ Best Parameters: {best_clf_params}\")\n",
    "print(f\"   ‚Ä¢ Cross-validation Accuracy: {best_clf_score:.4f}\")\n",
    "print(f\"   ‚Ä¢ Test Set Accuracy: {clf_test_accuracy:.4f}\")\n",
    "print(f\"   ‚Ä¢ Baseline Accuracy: {clf_baseline_accuracy:.4f}\")\n",
    "print(f\"   ‚Ä¢ Improvement: {clf_test_accuracy - clf_baseline_accuracy:.4f}\")\n",
    "print(f\"   ‚Ä¢ Relative Improvement: {((clf_test_accuracy - clf_baseline_accuracy) / clf_baseline_accuracy * 100):.2f}%\")\n",
    "\n",
    "print(f\"\\nüîç Model Architecture:\")\n",
    "print(f\"   ‚Ä¢ Number of Estimators: {len(best_clf_model.estimators_)}\")\n",
    "print(f\"   ‚Ä¢ Feature Importances Available: {len(best_clf_model.feature_importances_)}\")\n",
    "print(f\"   ‚Ä¢ Estimator Weights Shape: {best_clf_model.estimator_weights_.shape}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(f\"\\nüìã Detailed Classification Report:\")\n",
    "print(classification_report(y_clf_test, y_clf_pred_optimized, target_names=['Class 0', 'Class 1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442e5885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate Best Regression Model Performance\n",
    "\n",
    "print(\"\\nüéØ Best Regression Model Validation\")\n",
    "print(\"=\" * 42)\n",
    "\n",
    "# Make predictions with optimized regressor\n",
    "y_reg_pred_optimized = best_reg_model.predict(X_reg_test)\n",
    "\n",
    "# Create baseline for comparison\n",
    "baseline_reg = AdaBoostRegressor(random_state=42)\n",
    "baseline_reg.fit(X_reg_train, y_reg_train)\n",
    "y_reg_pred_baseline = baseline_reg.predict(X_reg_test)\n",
    "\n",
    "# Calculate comprehensive regression metrics\n",
    "reg_mse_optimized = mean_squared_error(y_reg_test, y_reg_pred_optimized)\n",
    "reg_rmse_optimized = np.sqrt(reg_mse_optimized)\n",
    "reg_mae_optimized = mean_absolute_error(y_reg_test, y_reg_pred_optimized) \n",
    "reg_r2_optimized = r2_score(y_reg_test, y_reg_pred_optimized)\n",
    "\n",
    "reg_mse_baseline = mean_squared_error(y_reg_test, y_reg_pred_baseline)\n",
    "reg_rmse_baseline = np.sqrt(reg_mse_baseline)\n",
    "reg_r2_baseline = r2_score(y_reg_test, y_reg_pred_baseline)\n",
    "\n",
    "print(f\"üìä Regression Performance Summary:\")\n",
    "print(f\"   ‚Ä¢ Best Parameters: {best_reg_params}\")\n",
    "print(f\"   ‚Ä¢ Cross-validation Score (neg_MSE): {best_reg_score:.4f}\")\n",
    "print(f\"   ‚Ä¢ Cross-validation RMSE: {np.sqrt(-best_reg_score):.4f}\")\n",
    "\n",
    "print(f\"\\nüìà Test Set Performance (Optimized vs Baseline):\")\n",
    "print(f\"   ‚Ä¢ MSE:  {reg_mse_optimized:.4f} vs {reg_mse_baseline:.4f}\")\n",
    "print(f\"   ‚Ä¢ RMSE: {reg_rmse_optimized:.4f} vs {reg_rmse_baseline:.4f}\")\n",
    "print(f\"   ‚Ä¢ MAE:  {reg_mae_optimized:.4f} vs {reg_mae_baseline:.4f}\")\n",
    "print(f\"   ‚Ä¢ R¬≤:   {reg_r2_optimized:.4f} vs {reg_r2_baseline:.4f}\")\n",
    "\n",
    "print(f\"\\nüí° Performance Improvements:\")\n",
    "print(f\"   ‚Ä¢ RMSE Reduction: {reg_rmse_baseline - reg_rmse_optimized:.4f}\")\n",
    "print(f\"   ‚Ä¢ R¬≤ Improvement: {reg_r2_optimized - reg_r2_baseline:.4f}\")\n",
    "print(f\"   ‚Ä¢ Relative RMSE Improvement: {((reg_rmse_baseline - reg_rmse_optimized) / reg_rmse_baseline * 100):.2f}%\")\n",
    "\n",
    "print(f\"\\nüîç Model Architecture:\")\n",
    "print(f\"   ‚Ä¢ Number of Estimators: {len(best_reg_model.estimators_)}\")\n",
    "print(f\"   ‚Ä¢ Feature Importances Available: {len(best_reg_model.feature_importances_)}\")\n",
    "print(f\"   ‚Ä¢ Estimator Weights Shape: {best_reg_model.estimator_weights_.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b831b7f",
   "metadata": {},
   "source": [
    "## 6. Parameter Impact Visualization\n",
    "\n",
    "Understanding how different parameters affect model performance helps in making informed tuning decisions. Let's create comprehensive visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbcaf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Parameter Performance Heatmaps\n",
    "\n",
    "def create_parameter_heatmap(grid_search, param1, param2, title):\n",
    "    \"\"\"Create heatmap showing parameter interaction effects.\"\"\"\n",
    "    \n",
    "    # Extract results from grid search\n",
    "    results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "    \n",
    "    # Create pivot table for heatmap\n",
    "    pivot_table = results_df.pivot_table(\n",
    "        values='mean_test_score',\n",
    "        index=f'param_{param1}',\n",
    "        columns=f'param_{param2}',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(pivot_table, annot=True, fmt='.4f', cmap='viridis', cbar_kws={'label': 'CV Score'})\n",
    "    plt.title(f'{title}\\nParameter Interaction Heatmap')\n",
    "    plt.xlabel(param2.replace('_', ' ').title())\n",
    "    plt.ylabel(param1.replace('_', ' ').title())\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Import pandas for data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "print(\"üìä Parameter Impact Visualization Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Classification parameter heatmap\n",
    "create_parameter_heatmap(\n",
    "    grid_search_clf, \n",
    "    'n_estimators', \n",
    "    'learning_rate',\n",
    "    'AdaBoost Classification'\n",
    ")\n",
    "\n",
    "# Algorithm comparison for classification\n",
    "results_clf = pd.DataFrame(grid_search_clf.cv_results_)\n",
    "algorithm_performance = results_clf.groupby('param_algorithm')['mean_test_score'].agg(['mean', 'std'])\n",
    "\n",
    "print(f\"\\nüîç Algorithm Performance Comparison (Classification):\")\n",
    "print(algorithm_performance.round(4))\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "algorithm_performance['mean'].plot(kind='bar', yerr=algorithm_performance['std'], capsize=4)\n",
    "plt.title('Algorithm Performance Comparison - Classification')\n",
    "plt.ylabel('Cross-Validation Accuracy')\n",
    "plt.xlabel('Algorithm')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97723333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression Parameter Analysis and Validation Curves\n",
    "\n",
    "print(\"\\nüìà Regression Parameter Analysis\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Regression parameter heatmap\n",
    "create_parameter_heatmap(\n",
    "    grid_search_reg,\n",
    "    'n_estimators', \n",
    "    'learning_rate',\n",
    "    'AdaBoost Regression'\n",
    ")\n",
    "\n",
    "# Loss function comparison for regression\n",
    "results_reg = pd.DataFrame(grid_search_reg.cv_results_)\n",
    "loss_performance = results_reg.groupby('param_loss')['mean_test_score'].agg(['mean', 'std'])\n",
    "\n",
    "print(f\"\\nüîç Loss Function Performance Comparison (Regression):\")\n",
    "print(\"Note: Scores are negative MSE (higher is better)\")\n",
    "print(loss_performance.round(4))\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "loss_performance['mean'].plot(kind='bar', yerr=loss_performance['std'], capsize=4)\n",
    "plt.title('Loss Function Performance Comparison - Regression')\n",
    "plt.ylabel('Cross-Validation Score (neg_MSE)')\n",
    "plt.xlabel('Loss Function')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create validation curves for key parameters\n",
    "print(\"\\nüìä Generating Validation Curves...\")\n",
    "\n",
    "# Validation curve for n_estimators (classification)\n",
    "estimator_range = [10, 25, 50, 75, 100, 150, 200]\n",
    "train_scores, test_scores = validation_curve(\n",
    "    AdaBoostClassifier(random_state=42), \n",
    "    X_clf_train, y_clf_train,\n",
    "    param_name='n_estimators',\n",
    "    param_range=estimator_range,\n",
    "    cv=5, scoring='accuracy', n_jobs=-1\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(estimator_range, train_scores.mean(axis=1), 'o-', label='Training Score')\n",
    "plt.plot(estimator_range, test_scores.mean(axis=1), 'o-', label='Validation Score')\n",
    "plt.fill_between(estimator_range, train_scores.mean(axis=1) - train_scores.std(axis=1),\n",
    "                 train_scores.mean(axis=1) + train_scores.std(axis=1), alpha=0.2)\n",
    "plt.fill_between(estimator_range, test_scores.mean(axis=1) - test_scores.std(axis=1),\n",
    "                 test_scores.mean(axis=1) + test_scores.std(axis=1), alpha=0.2)\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.title('Validation Curve - n_estimators (Classification)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Validation curve for learning_rate (classification)\n",
    "lr_range = [0.01, 0.1, 0.5, 1.0, 1.5, 2.0]\n",
    "train_scores_lr, test_scores_lr = validation_curve(\n",
    "    AdaBoostClassifier(random_state=42),\n",
    "    X_clf_train, y_clf_train,\n",
    "    param_name='learning_rate',\n",
    "    param_range=lr_range,\n",
    "    cv=5, scoring='accuracy', n_jobs=-1\n",
    ")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(lr_range, train_scores_lr.mean(axis=1), 'o-', label='Training Score')\n",
    "plt.plot(lr_range, test_scores_lr.mean(axis=1), 'o-', label='Validation Score')\n",
    "plt.fill_between(lr_range, train_scores_lr.mean(axis=1) - train_scores_lr.std(axis=1),\n",
    "                 train_scores_lr.mean(axis=1) + train_scores_lr.std(axis=1), alpha=0.2)\n",
    "plt.fill_between(lr_range, test_scores_lr.mean(axis=1) - test_scores_lr.std(axis=1),\n",
    "                 test_scores_lr.mean(axis=1) + test_scores_lr.std(axis=1), alpha=0.2)\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.title('Validation Curve - learning_rate (Classification)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Parameter impact analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacdfc53",
   "metadata": {},
   "source": [
    "## 7. Model Deployment and Prediction Pipeline\n",
    "\n",
    "Finally, let's create a complete prediction pipeline using our optimized models for real-world deployment scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8fa398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Production-Ready Prediction Pipeline\n",
    "\n",
    "class AdaBoostPredictionPipeline:\n",
    "    \"\"\"\n",
    "    Production-ready AdaBoost prediction pipeline with comprehensive functionality.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, classification_model=None, regression_model=None):\n",
    "        self.classification_model = classification_model\n",
    "        self.regression_model = regression_model\n",
    "        self.model_info = {}\n",
    "        \n",
    "        # Store model information\n",
    "        if classification_model:\n",
    "            self.model_info['classification'] = {\n",
    "                'parameters': classification_model.get_params(),\n",
    "                'n_estimators': len(classification_model.estimators_),\n",
    "                'feature_importances': classification_model.feature_importances_\n",
    "            }\n",
    "            \n",
    "        if regression_model:\n",
    "            self.model_info['regression'] = {\n",
    "                'parameters': regression_model.get_params(),\n",
    "                'n_estimators': len(regression_model.estimators_),\n",
    "                'feature_importances': regression_model.feature_importances_\n",
    "            }\n",
    "    \n",
    "    def predict_classification(self, X, return_probabilities=False):\n",
    "        \"\"\"Make classification predictions with optional probability scores.\"\"\"\n",
    "        if self.classification_model is None:\n",
    "            raise ValueError(\"No classification model available\")\n",
    "            \n",
    "        predictions = self.classification_model.predict(X)\n",
    "        \n",
    "        if return_probabilities:\n",
    "            probabilities = self.classification_model.predict_proba(X)\n",
    "            return predictions, probabilities\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def predict_regression(self, X):\n",
    "        \"\"\"Make regression predictions.\"\"\"\n",
    "        if self.regression_model is None:\n",
    "            raise ValueError(\"No regression model available\")\n",
    "            \n",
    "        return self.regression_model.predict(X)\n",
    "    \n",
    "    def get_feature_importance(self, task_type='classification'):\n",
    "        \"\"\"Get feature importance scores.\"\"\"\n",
    "        if task_type == 'classification' and self.classification_model:\n",
    "            return self.classification_model.feature_importances_\n",
    "        elif task_type == 'regression' and self.regression_model:\n",
    "            return self.regression_model.feature_importances_\n",
    "        else:\n",
    "            raise ValueError(f\"No {task_type} model available\")\n",
    "    \n",
    "    def model_summary(self):\n",
    "        \"\"\"Display comprehensive model summary.\"\"\"\n",
    "        print(\"üöÄ AdaBoost Prediction Pipeline Summary\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        for task_type, info in self.model_info.items():\n",
    "            print(f\"\\nüìä {task_type.title()} Model:\")\n",
    "            print(f\"   ‚Ä¢ Parameters: {info['parameters']}\")\n",
    "            print(f\"   ‚Ä¢ Number of Estimators: {info['n_estimators']}\")\n",
    "            print(f\"   ‚Ä¢ Feature Importance Available: {len(info['feature_importances'])} features\")\n",
    "\n",
    "# Initialize the production pipeline\n",
    "pipeline = AdaBoostPredictionPipeline(\n",
    "    classification_model=best_clf_model,\n",
    "    regression_model=best_reg_model\n",
    ")\n",
    "\n",
    "# Display pipeline summary\n",
    "pipeline.model_summary()\n",
    "\n",
    "print(\"\\n‚úÖ Production pipeline initialized successfully!\")\n",
    "print(\"üîß Available methods:\")\n",
    "print(\"   ‚Ä¢ predict_classification(X, return_probabilities=False)\")\n",
    "print(\"   ‚Ä¢ predict_regression(X)\")\n",
    "print(\"   ‚Ä¢ get_feature_importance(task_type)\")\n",
    "print(\"   ‚Ä¢ model_summary()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee91aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Pipeline Usage with Example Predictions\n",
    "\n",
    "print(\"üß™ Pipeline Demonstration with Example Predictions\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Generate sample data for demonstration\n",
    "n_samples = 5\n",
    "sample_clf_data = X_clf_test[:n_samples]\n",
    "sample_reg_data = X_reg_test[:n_samples]\n",
    "\n",
    "print(f\"üìù Making predictions on {n_samples} sample observations...\\n\")\n",
    "\n",
    "# Classification predictions\n",
    "print(\"üéØ Classification Predictions:\")\n",
    "print(\"-\" * 30)\n",
    "clf_predictions = pipeline.predict_classification(sample_clf_data)\n",
    "clf_predictions_proba = pipeline.predict_classification(sample_clf_data, return_probabilities=True)\n",
    "\n",
    "for i in range(n_samples):\n",
    "    actual = y_clf_test[i]\n",
    "    predicted = clf_predictions[i]\n",
    "    prob_class_0 = clf_predictions_proba[1][i][0]\n",
    "    prob_class_1 = clf_predictions_proba[1][i][1]\n",
    "    \n",
    "    print(f\"Sample {i+1}: Actual={actual}, Predicted={predicted}\")\n",
    "    print(f\"         Probabilities: Class 0={prob_class_0:.3f}, Class 1={prob_class_1:.3f}\")\n",
    "\n",
    "# Regression predictions\n",
    "print(f\"\\nüìà Regression Predictions:\")\n",
    "print(\"-\" * 25)\n",
    "reg_predictions = pipeline.predict_regression(sample_reg_data)\n",
    "\n",
    "for i in range(n_samples):\n",
    "    actual = y_reg_test[i]\n",
    "    predicted = reg_predictions[i]\n",
    "    error = abs(actual - predicted)\n",
    "    \n",
    "    print(f\"Sample {i+1}: Actual={actual:.3f}, Predicted={predicted:.3f}, Error={error:.3f}\")\n",
    "\n",
    "# Feature importance analysis\n",
    "print(f\"\\nüîç Feature Importance Analysis:\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "clf_importance = pipeline.get_feature_importance('classification')\n",
    "reg_importance = pipeline.get_feature_importance('regression')\n",
    "\n",
    "print(f\"Classification - Top 5 Important Features:\")\n",
    "top_clf_features = np.argsort(clf_importance)[-5:][::-1]\n",
    "for i, feature_idx in enumerate(top_clf_features):\n",
    "    print(f\"  {i+1}. Feature {feature_idx}: {clf_importance[feature_idx]:.4f}\")\n",
    "\n",
    "print(f\"\\nRegression - Top 5 Important Features:\")\n",
    "top_reg_features = np.argsort(reg_importance)[-5:][::-1]\n",
    "for i, feature_idx in enumerate(top_reg_features):\n",
    "    print(f\"  {i+1}. Feature {feature_idx}: {reg_importance[feature_idx]:.4f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Pipeline demonstration complete!\")\n",
    "print(f\"üöÄ Models are ready for production deployment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f5e0b3",
   "metadata": {},
   "source": [
    "## Summary and Key Insights\n",
    "\n",
    "This comprehensive AdaBoost implementation demonstrates advanced hyperparameter optimization techniques and production-ready deployment strategies.\n",
    "\n",
    "### üéØ Key Findings:\n",
    "\n",
    "**GridSearchCV Optimization:**\n",
    "- Systematic parameter search significantly improves model performance\n",
    "- Cross-validation provides robust performance estimates\n",
    "- Parameter interactions reveal optimal configurations\n",
    "\n",
    "**Parameter Impact Analysis:**\n",
    "- **n_estimators**: More estimators generally improve performance but increase overfitting risk\n",
    "- **learning_rate**: Lower rates provide better generalization, higher rates faster convergence\n",
    "- **algorithm/loss**: SAMME.R typically outperforms SAMME for classification\n",
    "\n",
    "**Production Deployment:**\n",
    "- Complete prediction pipeline with error handling\n",
    "- Feature importance analysis for model interpretability\n",
    "- Comprehensive performance monitoring capabilities\n",
    "\n",
    "### üöÄ Best Practices:\n",
    "\n",
    "1. **Hyperparameter Tuning Strategy:**\n",
    "   - Use GridSearchCV with cross-validation for robust optimization\n",
    "   - Monitor both training and validation performance\n",
    "   - Consider computational cost vs. performance trade-offs\n",
    "\n",
    "2. **Model Selection:**\n",
    "   - Start with reasonable parameter ranges\n",
    "   - Use validation curves to understand parameter sensitivity\n",
    "   - Balance model complexity with generalization ability\n",
    "\n",
    "3. **Production Deployment:**\n",
    "   - Implement comprehensive prediction pipelines\n",
    "   - Include error handling and model monitoring\n",
    "   - Maintain feature importance tracking for interpretability\n",
    "\n",
    "### üéì Educational Value:\n",
    "This notebook provides a complete workflow from basic implementation through advanced optimization to production deployment, demonstrating industry-standard practices for ensemble learning with AdaBoost."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
