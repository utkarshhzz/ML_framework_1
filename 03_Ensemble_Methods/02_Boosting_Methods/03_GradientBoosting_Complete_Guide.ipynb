{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e62d400",
   "metadata": {},
   "source": [
    "# Gradient Boosting Complete Implementation Guide\n",
    "\n",
    "## Overview\n",
    "Gradient Boosting is a powerful ensemble method that builds models sequentially, where each new model corrects errors made by previous models. This notebook provides comprehensive coverage of Gradient Boosting for both classification and regression tasks.\n",
    "\n",
    "## Algorithm Principles\n",
    "- **Sequential Model Building**: Each model trained to correct predecessor's errors\n",
    "- **Gradient Descent Optimization**: Uses gradients to minimize loss function\n",
    "- **Functional Gradient Descent**: Optimizes in function space rather than parameter space\n",
    "- **Residual Learning**: Each model learns from residuals of ensemble so far\n",
    "\n",
    "## Key Concepts Covered\n",
    "- **Mathematical Foundation**: Loss functions, gradients, and optimization\n",
    "- **Hyperparameter Analysis**: n_estimators, learning_rate, max_depth, subsample\n",
    "- **Advanced Techniques**: Early stopping, feature importance, regularization\n",
    "- **Performance Optimization**: GridSearchCV with comprehensive parameter grids\n",
    "- **Model Comparison**: Gradient Boosting vs AdaBoost vs Random Forest\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand Gradient Boosting mathematical foundations\n",
    "- Master comprehensive hyperparameter tuning strategies\n",
    "- Implement both classification and regression solutions\n",
    "- Analyze model performance with advanced visualization techniques\n",
    "- Deploy production-ready Gradient Boosting pipelines\n",
    "\n",
    "## Technical Stack\n",
    "- **scikit-learn**: GradientBoostingClassifier, GradientBoostingRegressor\n",
    "- **Optimization**: GridSearchCV, RandomizedSearchCV\n",
    "- **Visualization**: Advanced plotting for model analysis and interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d931255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Comprehensive Libraries for Gradient Boosting Analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.model_selection import (train_test_split, GridSearchCV, RandomizedSearchCV,\n",
    "                                   validation_curve, learning_curve, cross_val_score)\n",
    "from sklearn.ensemble import (GradientBoostingClassifier, GradientBoostingRegressor,\n",
    "                            AdaBoostClassifier, RandomForestClassifier)\n",
    "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix,\n",
    "                           mean_squared_error, mean_absolute_error, r2_score,\n",
    "                           roc_auc_score, roc_curve, precision_recall_curve)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure professional plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"viridis\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "\n",
    "print(\"üéØ Gradient Boosting Complete Analysis Environment\")\n",
    "print(\"=\" * 55)\n",
    "print(\"‚úÖ Core Libraries: NumPy, Pandas, Scikit-learn\")\n",
    "print(\"üìä Visualization: Matplotlib, Seaborn with professional styling\")\n",
    "print(\"üîß Models: GradientBoosting (Classification & Regression)\")\n",
    "print(\"‚ö° Optimization: Grid Search, Randomized Search, Cross-validation\")\n",
    "print(\"üìà Metrics: Comprehensive evaluation suite loaded\")\n",
    "print(\"\\nüöÄ Ready for advanced Gradient Boosting implementation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a49c6ff",
   "metadata": {},
   "source": [
    "## 1. Mathematical Foundation of Gradient Boosting\n",
    "\n",
    "Gradient Boosting builds an ensemble of weak learners sequentially, where each model is trained to minimize the residual errors of the combined ensemble.\n",
    "\n",
    "### Key Mathematical Concepts:\n",
    "- **Loss Function Minimization**: L(y, F(x)) where F(x) is the ensemble prediction\n",
    "- **Functional Gradient Descent**: Optimizes in function space\n",
    "- **Additive Model**: F_m(x) = F_{m-1}(x) + Œ≥_m * h_m(x)\n",
    "- **Residual Learning**: Each model learns -‚àáL(y, F_{m-1}(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dcb536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Comprehensive Datasets for Analysis\n",
    "\n",
    "print(\"üìä Creating Datasets for Gradient Boosting Analysis\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Classification Dataset - Complex multi-class problem\n",
    "X_clf, y_clf = make_classification(\n",
    "    n_samples=2000,          # Larger dataset for robust analysis\n",
    "    n_features=25,           # Higher dimensionality\n",
    "    n_informative=20,        # Most features are informative\n",
    "    n_redundant=3,           # Some redundant features\n",
    "    n_clusters_per_class=2,  # More complex class structure\n",
    "    n_classes=3,             # Multi-class classification\n",
    "    class_sep=0.8,           # Moderate class separation\n",
    "    random_state=42          # Reproducible results\n",
    ")\n",
    "\n",
    "# Regression Dataset - Nonlinear relationship\n",
    "X_reg, y_reg = make_regression(\n",
    "    n_samples=2000,          # Large dataset\n",
    "    n_features=20,           # Multiple features\n",
    "    n_informative=15,        # Most features informative\n",
    "    noise=0.15,              # Moderate noise level\n",
    "    random_state=42          # Reproducible results\n",
    ")\n",
    "\n",
    "# Split datasets with stratification for classification\n",
    "X_clf_train, X_clf_test, y_clf_train, y_clf_test = train_test_split(\n",
    "    X_clf, y_clf, test_size=0.25, random_state=42, stratify=y_clf\n",
    ")\n",
    "\n",
    "X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"üéØ Classification Dataset:\")\n",
    "print(f\"   ‚Ä¢ Training samples: {X_clf_train.shape[0]}\")\n",
    "print(f\"   ‚Ä¢ Testing samples: {X_clf_test.shape[0]}\")\n",
    "print(f\"   ‚Ä¢ Features: {X_clf_train.shape[1]}\")\n",
    "print(f\"   ‚Ä¢ Classes: {len(np.unique(y_clf))}\")\n",
    "print(f\"   ‚Ä¢ Class distribution: {dict(zip(*np.unique(y_clf, return_counts=True)))}\")\n",
    "\n",
    "print(f\"\\nüìà Regression Dataset:\")\n",
    "print(f\"   ‚Ä¢ Training samples: {X_reg_train.shape[0]}\")\n",
    "print(f\"   ‚Ä¢ Testing samples: {X_reg_test.shape[0]}\")\n",
    "print(f\"   ‚Ä¢ Features: {X_reg_train.shape[1]}\")\n",
    "print(f\"   ‚Ä¢ Target range: [{y_reg.min():.2f}, {y_reg.max():.2f}]\")\n",
    "print(f\"   ‚Ä¢ Target std: {y_reg.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df3d0a4",
   "metadata": {},
   "source": [
    "## 2. Gradient Boosting Parameters Deep Dive\n",
    "\n",
    "Understanding each parameter's impact is crucial for effective model tuning. Let's analyze the key parameters with mathematical insights and practical implications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7168772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Parameter Analysis for Gradient Boosting\n",
    "\n",
    "print(\"üîç Gradient Boosting Parameter Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define comprehensive parameter grids\n",
    "clf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],              # Number of boosting stages\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],     # Shrinkage parameter\n",
    "    'max_depth': [3, 4, 5, 6],                    # Tree depth (complexity)\n",
    "    'subsample': [0.8, 0.9, 1.0],                # Fraction of samples per tree\n",
    "    'min_samples_split': [2, 5, 10],              # Minimum samples to split\n",
    "    'min_samples_leaf': [1, 2, 4]                 # Minimum samples per leaf\n",
    "}\n",
    "\n",
    "reg_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],              # Number of boosting stages\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],     # Shrinkage parameter\n",
    "    'max_depth': [3, 4, 5, 6],                    # Tree depth\n",
    "    'subsample': [0.8, 0.9, 1.0],                # Stochastic gradient boosting\n",
    "    'min_samples_split': [2, 5, 10],              # Split threshold\n",
    "    'min_samples_leaf': [1, 2, 4],                # Leaf threshold\n",
    "    'loss': ['squared_error', 'absolute_error', 'huber']  # Loss functions\n",
    "}\n",
    "\n",
    "print(\"üìã CLASSIFICATION PARAMETERS:\")\n",
    "print(\"‚îÅ\" * 35)\n",
    "print(\"\\nüå≥ n_estimators (Boosting Stages):\")\n",
    "print(\"   ‚Ä¢ 100: Fast training, may underfit\")\n",
    "print(\"   ‚Ä¢ 200: Balanced performance/speed\")\n",
    "print(\"   ‚Ä¢ 300: Better accuracy, slower training\")\n",
    "print(\"   üí° More stages = lower bias, higher variance\")\n",
    "\n",
    "print(\"\\nüìà learning_rate (Shrinkage):\")\n",
    "print(\"   ‚Ä¢ 0.01: Very conservative, needs many estimators\")\n",
    "print(\"   ‚Ä¢ 0.05: Conservative, good generalization\")\n",
    "print(\"   ‚Ä¢ 0.1: Default, balanced approach\")\n",
    "print(\"   ‚Ä¢ 0.2: Aggressive, faster convergence\")\n",
    "print(\"   üí° Lower rate = better generalization, more estimators needed\")\n",
    "\n",
    "print(\"\\nüå≤ max_depth (Tree Complexity):\")\n",
    "print(\"   ‚Ä¢ 3: Simple trees, low variance\")\n",
    "print(\"   ‚Ä¢ 4-5: Moderate complexity, good balance\")\n",
    "print(\"   ‚Ä¢ 6+: Complex trees, high variance\")\n",
    "print(\"   üí° Deeper trees = more interactions captured\")\n",
    "\n",
    "print(\"\\nüé≤ subsample (Stochastic GB):\")\n",
    "print(\"   ‚Ä¢ 0.8: 80% of samples, reduces overfitting\")\n",
    "print(\"   ‚Ä¢ 0.9: 90% of samples, good balance\")\n",
    "print(\"   ‚Ä¢ 1.0: All samples, deterministic\")\n",
    "print(\"   üí° Subsampling introduces randomness, improves generalization\")\n",
    "\n",
    "print(\"\\nüìä REGRESSION LOSS FUNCTIONS:\")\n",
    "print(\"‚îÅ\" * 30)\n",
    "print(\"   ‚Ä¢ squared_error: L2 loss, sensitive to outliers\")\n",
    "print(\"   ‚Ä¢ absolute_error: L1 loss, robust to outliers\")\n",
    "print(\"   ‚Ä¢ huber: Combines L1 and L2, balanced approach\")\n",
    "\n",
    "# Calculate search space size\n",
    "clf_combinations = np.prod([len(values) for values in clf_param_grid.values()])\n",
    "reg_combinations = np.prod([len(values) for values in reg_param_grid.values()])\n",
    "\n",
    "print(f\"\\nüîç Search Space Analysis:\")\n",
    "print(f\"   ‚Ä¢ Classification combinations: {clf_combinations:,}\")\n",
    "print(f\"   ‚Ä¢ Regression combinations: {reg_combinations:,}\")\n",
    "print(f\"   ‚Ä¢ Total with 5-fold CV: {(clf_combinations + reg_combinations) * 5:,} fits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6b74b1",
   "metadata": {},
   "source": [
    "## 3. Advanced Hyperparameter Optimization\n",
    "\n",
    "We'll use both GridSearchCV and RandomizedSearchCV to demonstrate different optimization strategies and their trade-offs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4f471a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Comprehensive Gradient Boosting Classification Optimization\n",
    "\n",
    "print(\"üöÄ Starting Gradient Boosting Classification Optimization\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simplified parameter grid for demonstration (full grid would take very long)\n",
    "simplified_clf_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize Gradient Boosting Classifier\n",
    "gb_classifier = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Configure GridSearchCV\n",
    "print(\"‚öôÔ∏è Configuring GridSearchCV...\")\n",
    "grid_search_clf = GridSearchCV(\n",
    "    estimator=gb_classifier,\n",
    "    param_grid=simplified_clf_grid,\n",
    "    cv=5,                        # 5-fold cross-validation\n",
    "    scoring='accuracy',          # Optimization metric\n",
    "    n_jobs=-1,                   # Use all available cores\n",
    "    verbose=3                    # Detailed progress reporting\n",
    ")\n",
    "\n",
    "print(\"üîß Grid Search Configuration:\")\n",
    "print(f\"   ‚Ä¢ Parameter combinations: {np.prod([len(v) for v in simplified_clf_grid.values()])}\")\n",
    "print(f\"   ‚Ä¢ Cross-validation folds: 5\")\n",
    "print(f\"   ‚Ä¢ Total model fits: {np.prod([len(v) for v in simplified_clf_grid.values()]) * 5}\")\n",
    "\n",
    "print(\"\\n‚è≥ Executing grid search optimization...\")\n",
    "print(\"   This may take several minutes depending on your system...\\n\")\n",
    "\n",
    "# Execute the grid search\n",
    "grid_search_clf.fit(X_clf_train, y_clf_train)\n",
    "\n",
    "print(f\"\\n‚úÖ Classification Optimization Complete!\")\n",
    "print(f\"üèÜ Best Parameters: {grid_search_clf.best_params_}\")\n",
    "print(f\"üìä Best CV Accuracy: {grid_search_clf.best_score_:.4f}\")\n",
    "print(f\"‚ö° Total fits completed: {len(grid_search_clf.cv_results_['mean_test_score'])}\")\n",
    "\n",
    "# Store results for analysis\n",
    "best_clf_gb = grid_search_clf.best_estimator_\n",
    "best_clf_params = grid_search_clf.best_params_\n",
    "best_clf_score = grid_search_clf.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67d7786",
   "metadata": {},
   "source": [
    "## 4. Model Performance Analysis and Comparison\n",
    "\n",
    "Let's evaluate our optimized model and compare it with baseline implementations to understand the improvement gained through hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dea7f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Model Evaluation and Comparison\n",
    "\n",
    "print(\"üéØ Comprehensive Model Performance Evaluation\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Create baseline model for comparison\n",
    "baseline_gb = GradientBoostingClassifier(random_state=42)\n",
    "baseline_gb.fit(X_clf_train, y_clf_train)\n",
    "\n",
    "# Make predictions with both models\n",
    "y_pred_optimized = best_clf_gb.predict(X_clf_test)\n",
    "y_pred_baseline = baseline_gb.predict(X_clf_test)\n",
    "\n",
    "# Calculate performance metrics\n",
    "optimized_accuracy = accuracy_score(y_clf_test, y_pred_optimized)\n",
    "baseline_accuracy = accuracy_score(y_clf_test, y_pred_baseline)\n",
    "\n",
    "print(\"üìä Performance Comparison:\")\n",
    "print(\"‚îÅ\" * 30)\n",
    "print(f\"üèÜ Optimized Model:\")\n",
    "print(f\"   ‚Ä¢ Best Parameters: {best_clf_params}\")\n",
    "print(f\"   ‚Ä¢ CV Score: {best_clf_score:.4f}\")\n",
    "print(f\"   ‚Ä¢ Test Accuracy: {optimized_accuracy:.4f}\")\n",
    "print(f\"   ‚Ä¢ Number of Estimators: {best_clf_gb.n_estimators}\")\n",
    "\n",
    "print(f\"\\nüìù Baseline Model:\")\n",
    "print(f\"   ‚Ä¢ Default Parameters: n_estimators=100, learning_rate=0.1\")\n",
    "print(f\"   ‚Ä¢ Test Accuracy: {baseline_accuracy:.4f}\")\n",
    "\n",
    "print(f\"\\nüí° Improvement Analysis:\")\n",
    "improvement = optimized_accuracy - baseline_accuracy\n",
    "relative_improvement = (improvement / baseline_accuracy) * 100\n",
    "print(f\"   ‚Ä¢ Absolute Improvement: {improvement:.4f}\")\n",
    "print(f\"   ‚Ä¢ Relative Improvement: {relative_improvement:.2f}%\")\n",
    "\n",
    "# Detailed classification reports\n",
    "print(f\"\\nüìã Detailed Classification Report (Optimized):\")\n",
    "print(\"=\" * 50)\n",
    "print(classification_report(y_clf_test, y_pred_optimized, target_names=[f'Class {i}' for i in range(3)]))\n",
    "\n",
    "# Feature importance analysis\n",
    "feature_importance = best_clf_gb.feature_importances_\n",
    "top_features = np.argsort(feature_importance)[-10:][::-1]\n",
    "\n",
    "print(f\"\\nüîç Top 10 Feature Importances:\")\n",
    "print(\"‚îÅ\" * 35)\n",
    "for i, feature_idx in enumerate(top_features):\n",
    "    print(f\"{i+1:2d}. Feature {feature_idx:2d}: {feature_importance[feature_idx]:.4f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Model evaluation complete!\")\n",
    "print(f\"üöÄ Optimized Gradient Boosting ready for deployment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28214a4b",
   "metadata": {},
   "source": [
    "## Summary and Key Insights\n",
    "\n",
    "This comprehensive Gradient Boosting implementation demonstrates advanced ensemble learning techniques with systematic optimization and thorough performance analysis.\n",
    "\n",
    "### üéØ Key Findings:\n",
    "\n",
    "**Hyperparameter Optimization:**\n",
    "- Systematic grid search significantly improves model performance\n",
    "- Parameter interactions are crucial for optimal results\n",
    "- Cross-validation provides robust performance estimates\n",
    "\n",
    "**Parameter Impact Analysis:**\n",
    "- **n_estimators**: More stages generally improve performance but increase training time\n",
    "- **learning_rate**: Lower rates with more estimators often yield better generalization\n",
    "- **max_depth**: Moderate depth (3-5) balances bias-variance trade-off effectively\n",
    "- **subsample**: Stochastic gradient boosting reduces overfitting\n",
    "\n",
    "**Performance Insights:**\n",
    "- Gradient Boosting excels at capturing complex patterns and interactions\n",
    "- Feature importance provides valuable insights for model interpretation\n",
    "- Proper regularization through learning_rate and subsample is crucial\n",
    "\n",
    "### üöÄ Best Practices:\n",
    "\n",
    "1. **Hyperparameter Strategy:**\n",
    "   - Start with moderate learning rates (0.05-0.1)\n",
    "   - Use more estimators with lower learning rates\n",
    "   - Balance model complexity with computational resources\n",
    "\n",
    "2. **Model Selection:**\n",
    "   - Use cross-validation for robust parameter selection\n",
    "   - Consider early stopping to prevent overfitting\n",
    "   - Monitor both training and validation performance\n",
    "\n",
    "3. **Production Deployment:**\n",
    "   - Implement comprehensive monitoring and validation\n",
    "   - Consider model update strategies for changing data distributions\n",
    "   - Balance prediction accuracy with computational efficiency\n",
    "\n",
    "### üéì Educational Value:\n",
    "This notebook demonstrates industry-standard practices for gradient boosting, from mathematical foundations through advanced optimization to production considerations."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
