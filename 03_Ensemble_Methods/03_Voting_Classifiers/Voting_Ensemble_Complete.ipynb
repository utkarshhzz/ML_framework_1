{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ed896cd",
   "metadata": {},
   "source": [
    "# ğŸ¯ Ensemble Methods: Voting Classifier Fundamentals\n",
    "\n",
    "## ğŸ“š Complete Guide to Ensemble Learning and Voting Classifiers\n",
    "\n",
    "### ğŸ“– Table of Contents:\n",
    "1. **[ğŸ”¬ Theory Introduction](#theory)** - Ensemble learning concepts\n",
    "2. **[ğŸ“Š Dataset Creation](#dataset)** - Understanding make_classification parameters\n",
    "3. **[ğŸ¤– Individual Classifiers](#classifiers)** - Building base models\n",
    "4. **[ğŸ—³ï¸ Voting Classifier](#voting)** - Combining classifiers for better performance\n",
    "5. **[ğŸ“ˆ Performance Evaluation](#evaluation)** - Measuring ensemble effectiveness\n",
    "6. **[ğŸ§  Key Insights](#insights)** - Understanding why ensembles work\n",
    "\n",
    "### ğŸ“ What You'll Learn:\n",
    "1. **Ensemble Theory**: Why multiple models outperform single models\n",
    "2. **Dataset Parameters**: Deep understanding of `make_classification` parameters\n",
    "3. **Base Classifiers**: Naive Bayes, Logistic Regression, Decision Trees, SVM\n",
    "4. **Voting Mechanisms**: Hard vs Soft voting strategies\n",
    "5. **Performance Analysis**: Comparing individual vs ensemble performance\n",
    "\n",
    "### ğŸ§  Core Concepts:\n",
    "- **Ensemble Learning**: Combining multiple models for improved predictions\n",
    "- **Voting Classifier**: Democratic approach to classification\n",
    "- **Bias-Variance Tradeoff**: How ensembles reduce both bias and variance\n",
    "- **Diversity**: Why different algorithms complement each other\n",
    "\n",
    "### â±ï¸ Estimated Reading Time: 20-25 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2016a5be",
   "metadata": {},
   "source": [
    "## ğŸ”¬ Theory Introduction: Why Ensemble Methods Work\n",
    "\n",
    "### ğŸ§  The Fundamental Principle:\n",
    "**\"The wisdom of crowds\"** - Multiple experts making decisions together often outperform any single expert.\n",
    "\n",
    "### ğŸ¯ Mathematical Foundation:\n",
    "\n",
    "If we have **n** classifiers, each with accuracy **p > 0.5**, the probability that the majority vote is correct:\n",
    "\n",
    "**P(majority correct) = Î£ C(n,k) Ã— p^k Ã— (1-p)^(n-k)**\n",
    "\n",
    "Where k > n/2 (majority)\n",
    "\n",
    "### ğŸ“Š Why This Works:\n",
    "1. **Error Reduction**: Different models make different types of errors\n",
    "2. **Variance Reduction**: Averaging reduces prediction variance\n",
    "3. **Bias Mitigation**: Combining diverse models can reduce systematic bias\n",
    "4. **Robustness**: Less sensitive to outliers and noise\n",
    "\n",
    "### ğŸ” Types of Ensemble Methods:\n",
    "1. **Voting/Averaging**: Simple combination of predictions\n",
    "2. **Bagging**: Bootstrap Aggregating (Random Forest)\n",
    "3. **Boosting**: Sequential learning (AdaBoost, XGBoost)\n",
    "4. **Stacking**: Meta-learning approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dc41e6",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Step 1: Import Required Libraries\n",
    "\n",
    "We'll use scikit-learn's comprehensive suite for ensemble learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8332158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries for ensemble learning\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Dataset creation and preprocessing\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Individual classifiers (our \"experts\")\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Ensemble methods\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(\"ğŸ¯ Ready to explore ensemble learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e98770",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 2: Dataset Creation with make_classification\n",
    "\n",
    "### ğŸ§® Understanding make_classification Parameters:\n",
    "\n",
    "Let's create a synthetic dataset and understand every parameter in detail:\n",
    "\n",
    "#### ğŸ” Parameter Deep Dive:\n",
    "\n",
    "1. **`n_samples=1000`**: \n",
    "   - **What**: Total number of data points to generate\n",
    "   - **Why**: 1000 provides sufficient data for reliable training and testing\n",
    "   - **Impact**: More samples = more robust model training\n",
    "\n",
    "2. **`n_features=20`**: \n",
    "   - **What**: Number of input features (dimensions)\n",
    "   - **Why**: 20 features create moderate complexity without curse of dimensionality\n",
    "   - **Impact**: More features = more complex decision boundaries\n",
    "\n",
    "3. **`n_classes=2`**: \n",
    "   - **What**: Number of target classes (binary classification)\n",
    "   - **Why**: Binary problems are easier to understand and visualize\n",
    "   - **Options**: Can be 2, 3, 4, etc. for multi-class problems\n",
    "   - **Impact**: More classes = more complex classification task\n",
    "\n",
    "4. **`random_state=1`**: \n",
    "   - **What**: Seed for random number generator\n",
    "   - **Why**: Ensures reproducible results across runs\n",
    "   - **Impact**: Same seed = identical dataset every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bdb513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ² Create synthetic classification dataset\n",
    "print(\"ğŸ”¬ Creating Synthetic Classification Dataset\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Generate the dataset with detailed parameters\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,        # Total number of samples\n",
    "    n_features=20,         # Number of input features\n",
    "    n_classes=2,           # Binary classification (0 and 1)\n",
    "    n_informative=15,      # Number of informative features\n",
    "    n_redundant=3,         # Number of redundant features\n",
    "    n_clusters_per_class=1, # Number of clusters per class\n",
    "    class_sep=1.0,         # Factor multiplying the hypercube size\n",
    "    random_state=1         # For reproducibility\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“Š Dataset Created Successfully!\")\n",
    "print(f\"ğŸ“ Feature matrix shape: {X.shape}\")\n",
    "print(f\"ğŸ¯ Target vector shape: {y.shape}\")\n",
    "print(f\"ğŸ“ˆ Features: {X.shape[1]} dimensions\")\n",
    "print(f\"ğŸ”¢ Samples: {X.shape[0]} data points\")\n",
    "print(f\"ğŸ·ï¸ Classes: {len(np.unique(y))} unique classes\")\n",
    "print(f\"ğŸ“Š Class distribution: {np.bincount(y)}\")\n",
    "print(f\"âš–ï¸ Class balance: {np.bincount(y) / len(y) * 100}%\")\n",
    "\n",
    "# Additional dataset insights\n",
    "print(\"\\nğŸ” Dataset Characteristics:\")\n",
    "print(f\"ğŸ“Š Feature value range: [{X.min():.2f}, {X.max():.2f}]\")\n",
    "print(f\"ğŸ“ˆ Feature mean: {X.mean():.2f}\")\n",
    "print(f\"ğŸ“‰ Feature std: {X.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b388aca",
   "metadata": {},
   "source": [
    "### ğŸ§  Advanced make_classification Parameters Explained:\n",
    "\n",
    "#### ğŸ¯ **n_informative vs n_redundant vs n_repeated**:\n",
    "\n",
    "1. **`n_informative=15`**: \n",
    "   - **Features that actually help** in classification\n",
    "   - These features have **genuine predictive power**\n",
    "   - **Example**: In medical diagnosis, these would be symptoms that actually indicate disease\n",
    "\n",
    "2. **`n_redundant=3`**: \n",
    "   - **Linear combinations** of informative features\n",
    "   - Add **multicollinearity** but no new information\n",
    "   - **Example**: If height and weight are informative, BMI would be redundant\n",
    "\n",
    "3. **`n_repeated=0`** (default): \n",
    "   - **Exact duplicates** of existing features\n",
    "   - Pure noise that adds no value\n",
    "\n",
    "#### ğŸ¨ **class_sep Parameter**:\n",
    "- **What**: Controls how **separable** the classes are\n",
    "- **Range**: Higher values = easier separation\n",
    "- **Impact**: class_sep=0.5 â†’ overlapping classes, class_sep=2.0 â†’ clearly separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9d8307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”ª Split the dataset for training and testing\n",
    "print(\"ğŸ”€ Splitting Dataset for Training and Testing\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,                    # Our features and targets\n",
    "    test_size=0.2,          # 20% for testing, 80% for training\n",
    "    random_state=1,         # Reproducible splits\n",
    "    stratify=y              # Maintain class proportions\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“ Training set shape: {X_train.shape}\")\n",
    "print(f\"ğŸ§ª Testing set shape: {X_test.shape}\")\n",
    "print(f\"ğŸ“Š Training samples: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"ğŸ”¬ Testing samples: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Verify stratification worked\n",
    "print(f\"\\nâš–ï¸ Class Distribution Verification:\")\n",
    "print(f\"ğŸ“ˆ Original: {np.bincount(y) / len(y) * 100}%\")\n",
    "print(f\"ğŸ“ Training: {np.bincount(y_train) / len(y_train) * 100}%\")\n",
    "print(f\"ğŸ§ª Testing: {np.bincount(y_test) / len(y_test) * 100}%\")\n",
    "print(\"âœ… Stratification successful - proportions maintained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffa8fd1",
   "metadata": {},
   "source": [
    "## ğŸ¤– Step 3: Individual Classifiers - Our \"Expert Panel\"\n",
    "\n",
    "### ğŸ§  Why These Four Algorithms?\n",
    "\n",
    "We're selecting four fundamentally different algorithms to maximize **diversity**:\n",
    "\n",
    "#### ğŸ¯ **Algorithm Diversity Analysis**:\n",
    "\n",
    "1. **ğŸ”¢ Naive Bayes (Probabilistic)**:\n",
    "   - **Assumption**: Features are independent given the class\n",
    "   - **Strength**: Fast, works well with small datasets\n",
    "   - **Weakness**: Independence assumption often violated\n",
    "   - **Best for**: Text classification, categorical features\n",
    "\n",
    "2. **ğŸ“ˆ Logistic Regression (Linear)**:\n",
    "   - **Assumption**: Linear relationship between features and log-odds\n",
    "   - **Strength**: Interpretable, provides probabilities\n",
    "   - **Weakness**: Assumes linear decision boundary\n",
    "   - **Best for**: Linear separable problems, feature importance\n",
    "\n",
    "3. **ğŸŒ³ Decision Tree (Non-linear)**:\n",
    "   - **Assumption**: Data can be split using feature thresholds\n",
    "   - **Strength**: Handles non-linear patterns, interpretable\n",
    "   - **Weakness**: Prone to overfitting, unstable\n",
    "   - **Best for**: Non-linear patterns, mixed data types\n",
    "\n",
    "4. **ğŸ¯ SVM (Geometric)**:\n",
    "   - **Assumption**: Optimal separating hyperplane exists\n",
    "   - **Strength**: Effective in high dimensions, memory efficient\n",
    "   - **Weakness**: Slow on large datasets, requires feature scaling\n",
    "   - **Best for**: High-dimensional data, robust classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e507a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¤– Initialize our \"Expert Panel\" of classifiers\n",
    "print(\"ğŸ¤– Assembling Expert Panel of Classifiers\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "# Create individual classifiers with optimal parameters\n",
    "print(\"ğŸ”¢ Expert 1: Naive Bayes (Probabilistic Approach)\")\n",
    "nb_clf = GaussianNB()\n",
    "print(\"   ğŸ“Š Assumes: Feature independence given class\")\n",
    "print(\"   ğŸ¯ Strength: Fast, probabilistic outputs\")\n",
    "\n",
    "print(\"\\nğŸ“ˆ Expert 2: Logistic Regression (Linear Approach)\")\n",
    "lr_clf = LogisticRegression(random_state=1, max_iter=1000)\n",
    "print(\"   ğŸ“Š Assumes: Linear decision boundary\")\n",
    "print(\"   ğŸ¯ Strength: Interpretable, stable\")\n",
    "\n",
    "print(\"\\nğŸŒ³ Expert 3: Decision Tree (Rule-based Approach)\")\n",
    "dt_clf = DecisionTreeClassifier(random_state=1, max_depth=10)\n",
    "print(\"   ğŸ“Š Assumes: Hierarchical feature splits\")\n",
    "print(\"   ğŸ¯ Strength: Non-linear, interpretable\")\n",
    "\n",
    "print(\"\\nğŸ¯ Expert 4: Support Vector Machine (Geometric Approach)\")\n",
    "svm_clf = SVC(kernel='linear', random_state=1)\n",
    "print(\"   ğŸ“Š Assumes: Optimal separating hyperplane\")\n",
    "print(\"   ğŸ¯ Strength: Robust, high-dimensional\")\n",
    "\n",
    "print(\"\\nâœ… Expert Panel Assembled!\")\n",
    "print(\"ğŸ­ Four diverse algorithms ready for ensemble\")\n",
    "\n",
    "# Store classifiers for easy reference\n",
    "classifiers = {\n",
    "    'Naive Bayes': nb_clf,\n",
    "    'Logistic Regression': lr_clf,\n",
    "    'Decision Tree': dt_clf,\n",
    "    'SVM': svm_clf\n",
    "}\n",
    "\n",
    "print(f\"\\nğŸ“‹ Expert Panel Summary: {len(classifiers)} diverse algorithms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd28306",
   "metadata": {},
   "source": [
    "## ğŸ—³ï¸ Step 4: The Voting Classifier - Democratic Decision Making\n",
    "\n",
    "### ğŸ§  Voting Mechanisms Explained:\n",
    "\n",
    "#### ğŸ—³ï¸ **Hard Voting (Majority Rule)**:\n",
    "- **Process**: Each classifier gives a class prediction (0 or 1)\n",
    "- **Decision**: Majority vote wins\n",
    "- **Example**: \n",
    "  - NB predicts: 1, LR predicts: 0, DT predicts: 1, SVM predicts: 1\n",
    "  - **Result**: Class 1 (3 votes vs 1 vote)\n",
    "\n",
    "#### ğŸ¯ **Soft Voting (Probability-based)**:\n",
    "- **Process**: Each classifier gives probability estimates\n",
    "- **Decision**: Average probabilities, choose highest\n",
    "- **Example**: \n",
    "  - NB: [0.2, 0.8], LR: [0.6, 0.4], DT: [0.3, 0.7], SVM: [0.4, 0.6]\n",
    "  - **Average**: [0.375, 0.625] â†’ **Result**: Class 1\n",
    "\n",
    "#### ğŸª **Why Voting Works**:\n",
    "1. **Error Compensation**: Different algorithms make different mistakes\n",
    "2. **Confidence Weighting**: More confident predictions have more impact\n",
    "3. **Stability**: Reduces variance of individual predictions\n",
    "4. **Robustness**: Less sensitive to outliers or noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58708a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ—³ï¸ Create the Voting Classifier - Our Democratic Ensemble\n",
    "print(\"ğŸ—³ï¸ Creating Democratic Voting Classifier\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Define the ensemble with descriptive names\n",
    "ensemble_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('naive_bayes', nb_clf),           # Probabilistic expert\n",
    "        ('logistic_regression', lr_clf),   # Linear expert\n",
    "        ('decision_tree', dt_clf),         # Non-linear expert\n",
    "        ('svm', svm_clf)                   # Geometric expert\n",
    "    ],\n",
    "    voting='hard'  # Majority voting (can be 'soft' for probability averaging)\n",
    ")\n",
    "\n",
    "print(f\"ğŸ­ Ensemble Configuration:\")\n",
    "print(f\"   ğŸ‘¥ Number of experts: {len(ensemble_clf.estimators)}\")\n",
    "print(f\"   ğŸ—³ï¸ Voting method: {ensemble_clf.voting.upper()}\")\n",
    "print(f\"   ğŸ¯ Decision rule: Majority vote wins\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ Expert Panel Members:\")\n",
    "for name, clf in ensemble_clf.estimators:\n",
    "    print(f\"   ğŸ¤– {name}: {type(clf).__name__}\")\n",
    "\n",
    "print(f\"\\nğŸ§  Voting Logic:\")\n",
    "print(f\"   ğŸ“Š Each expert gives class prediction (0 or 1)\")\n",
    "print(f\"   ğŸ—³ï¸ Final prediction = majority vote\")\n",
    "print(f\"   ğŸ¯ Ties resolved by first classifier in order\")\n",
    "\n",
    "print(f\"\\nâœ… Democratic Ensemble Ready!\")\n",
    "print(f\"ğŸª Four diverse experts working together\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8cbd9d",
   "metadata": {},
   "source": [
    "### ğŸ”¬ Training Phase: Teaching Our Expert Panel\n",
    "\n",
    "When we train the voting classifier, here's what happens:\n",
    "\n",
    "1. **ğŸ“ Individual Training**: Each base classifier learns from the training data\n",
    "2. **ğŸ§  Pattern Learning**: Each algorithm discovers different patterns\n",
    "3. **ğŸ¯ Specialization**: Each expert becomes good at different aspects\n",
    "4. **ğŸ¤ Integration**: Voting mechanism prepares to combine their expertise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7df4ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“ Train the ensemble - Teaching our expert panel\n",
    "print(\"ğŸ“ Training the Expert Panel\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "print(\"ğŸ“š Training individual experts...\")\n",
    "start_time = np.datetime64('now')\n",
    "\n",
    "# Train the ensemble (automatically trains all base classifiers)\n",
    "ensemble_clf.fit(X_train, y_train)\n",
    "\n",
    "end_time = np.datetime64('now')\n",
    "print(f\"â±ï¸ Training completed!\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Training Summary:\")\n",
    "print(f\"   ğŸ“Š Training samples: {len(X_train)}\")\n",
    "print(f\"   ğŸ”¢ Features per sample: {X_train.shape[1]}\")\n",
    "print(f\"   ğŸ­ Experts trained: {len(ensemble_clf.estimators)}\")\n",
    "\n",
    "print(f\"\\nâœ… Expert Panel Successfully Trained!\")\n",
    "print(f\"ğŸ¤– All {len(ensemble_clf.estimators)} experts ready for predictions\")\n",
    "print(f\"ğŸ—³ï¸ Democratic voting mechanism activated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f784641",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ Step 5: Making Predictions - The Democratic Process\n",
    "\n",
    "### ğŸ—³ï¸ How Ensemble Prediction Works:\n",
    "\n",
    "For each test sample:\n",
    "1. **ğŸ¤– Individual Predictions**: Each expert makes its prediction\n",
    "2. **ğŸ—³ï¸ Vote Collection**: Gather all expert opinions\n",
    "3. **ğŸ“Š Vote Counting**: Count votes for each class\n",
    "4. **ğŸ¯ Final Decision**: Majority vote determines final prediction\n",
    "\n",
    "**Example Process:**\n",
    "- Sample X: [feature1=2.1, feature2=-0.5, ...]\n",
    "- Expert votes: NBâ†’1, LRâ†’0, DTâ†’1, SVMâ†’1\n",
    "- Vote count: Class 0 = 1 vote, Class 1 = 3 votes\n",
    "- **Final prediction: Class 1** âœ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6330f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”® Make predictions using our democratic ensemble\n",
    "print(\"ğŸ”® Making Predictions with Democratic Ensemble\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "print(\"ğŸ—³ï¸ Collecting votes from expert panel...\")\n",
    "\n",
    "# Get ensemble predictions\n",
    "y_pred_ensemble = ensemble_clf.predict(X_test)\n",
    "\n",
    "print(f\"âœ… Predictions completed!\")\n",
    "print(f\"   ğŸ“Š Test samples processed: {len(X_test)}\")\n",
    "print(f\"   ğŸ­ Experts consulted per sample: {len(ensemble_clf.estimators)}\")\n",
    "print(f\"   ğŸ—³ï¸ Total votes cast: {len(X_test) * len(ensemble_clf.estimators)}\")\n",
    "\n",
    "# Show sample predictions\n",
    "print(f\"\\nğŸ“‹ Sample Predictions (first 10):\")\n",
    "print(f\"   ğŸ”® Ensemble: {y_pred_ensemble[:10]}\")\n",
    "print(f\"   ğŸ¯ Actual:   {y_test[:10]}\")\n",
    "\n",
    "# Quick accuracy check\n",
    "quick_accuracy = accuracy_score(y_test, y_pred_ensemble)\n",
    "print(f\"\\nğŸ¯ Quick Accuracy Check: {quick_accuracy:.4f} ({quick_accuracy*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705867c8",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 6: Individual Expert Performance Analysis\n",
    "\n",
    "Let's see how each expert performs individually before combining them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93c6e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š Evaluate individual expert performance\n",
    "print(\"ğŸ“Š Individual Expert Performance Analysis\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "individual_scores = {}\n",
    "\n",
    "print(\"ğŸ” Testing each expert individually...\")\n",
    "print()\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    # Train individual classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_individual = clf.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred_individual)\n",
    "    individual_scores[name] = accuracy\n",
    "    \n",
    "    print(f\"ğŸ¤– {name}:\")\n",
    "    print(f\"   ğŸ¯ Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print()\n",
    "\n",
    "# Calculate ensemble accuracy\n",
    "ensemble_accuracy = accuracy_score(y_test, y_pred_ensemble)\n",
    "individual_scores['Ensemble'] = ensemble_accuracy\n",
    "\n",
    "print(f\"ğŸ—³ï¸ Democratic Ensemble:\")\n",
    "print(f\"   ğŸ¯ Accuracy: {ensemble_accuracy:.4f} ({ensemble_accuracy*100:.2f}%)\")\n",
    "print()\n",
    "\n",
    "# Find best individual performer\n",
    "best_individual = max(individual_scores.items(), key=lambda x: x[1] if x[0] != 'Ensemble' else 0)\n",
    "print(f\"ğŸ† Best Individual Expert: {best_individual[0]} ({best_individual[1]*100:.2f}%)\")\n",
    "\n",
    "# Calculate improvement\n",
    "improvement = ensemble_accuracy - best_individual[1]\n",
    "print(f\"ğŸ“ˆ Ensemble Improvement: {improvement:+.4f} ({improvement*100:+.2f} percentage points)\")\n",
    "\n",
    "if improvement > 0:\n",
    "    print(\"âœ… Ensemble outperforms best individual expert!\")\n",
    "else:\n",
    "    print(\"âš ï¸ Individual expert outperforms ensemble (possible overfitting)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e191fd7",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ Step 7: Comprehensive Performance Evaluation\n",
    "\n",
    "### ğŸ¯ Evaluation Metrics Explained:\n",
    "\n",
    "1. **ğŸ“Š Accuracy**: Overall correctness (TP+TN)/(TP+TN+FP+FN)\n",
    "2. **ğŸ¯ Precision**: How many predicted positives were actually positive\n",
    "3. **ğŸ“ˆ Recall**: How many actual positives were correctly identified\n",
    "4. **âš–ï¸ F1-Score**: Harmonic mean of precision and recall\n",
    "5. **ğŸª Confusion Matrix**: Detailed breakdown of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4fe4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“ˆ Comprehensive performance evaluation\n",
    "print(\"ğŸ“ˆ Comprehensive Ensemble Performance Evaluation\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Calculate final accuracy\n",
    "final_accuracy = accuracy_score(y_test, y_pred_ensemble)\n",
    "print(f\"ğŸ¯ Final Ensemble Accuracy: {final_accuracy:.4f} ({final_accuracy*100:.2f}%)\")\n",
    "print()\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"ğŸ“Š Detailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_ensemble, target_names=['Class 0', 'Class 1']))\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"ğŸª Confusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred_ensemble)\n",
    "print(cm)\n",
    "print()\n",
    "\n",
    "# Confusion matrix interpretation\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"ğŸ” Confusion Matrix Breakdown:\")\n",
    "print(f\"   âœ… True Negatives (Correct Class 0): {tn}\")\n",
    "print(f\"   âŒ False Positives (Wrong Class 1): {fp}\")\n",
    "print(f\"   âŒ False Negatives (Missed Class 1): {fn}\")\n",
    "print(f\"   âœ… True Positives (Correct Class 1): {tp}\")\n",
    "print()\n",
    "\n",
    "# Calculate additional metrics\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(f\"ğŸ“Š Manual Metric Verification:\")\n",
    "print(f\"   ğŸ¯ Precision: {precision:.4f}\")\n",
    "print(f\"   ğŸ“ˆ Recall: {recall:.4f}\")\n",
    "print(f\"   âš–ï¸ F1-Score: {f1:.4f}\")\n",
    "print(f\"   ğŸ“Š Accuracy: {(tp + tn) / (tp + tn + fp + fn):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6e4393",
   "metadata": {},
   "source": [
    "## ğŸ¨ Step 8: Performance Visualization\n",
    "\n",
    "Let's create visualizations to better understand our ensemble's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a409df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¨ Performance visualization\n",
    "print(\"ğŸ¨ Creating Performance Visualizations\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('ğŸ—³ï¸ Ensemble Learning Performance Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Individual vs Ensemble Accuracy Comparison\n",
    "ax1 = axes[0, 0]\n",
    "names = list(individual_scores.keys())\n",
    "scores = list(individual_scores.values())\n",
    "colors = ['skyblue', 'lightgreen', 'lightcoral', 'lightsalmon', 'gold']\n",
    "\n",
    "bars = ax1.bar(names, scores, color=colors)\n",
    "ax1.set_title('ğŸ† Individual vs Ensemble Performance', fontweight='bold')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars, scores):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 2. Confusion Matrix Heatmap\n",
    "ax2 = axes[0, 1]\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Predicted 0', 'Predicted 1'],\n",
    "            yticklabels=['Actual 0', 'Actual 1'], ax=ax2)\n",
    "ax2.set_title('ğŸª Confusion Matrix', fontweight='bold')\n",
    "\n",
    "# 3. Class Distribution\n",
    "ax3 = axes[1, 0]\n",
    "class_counts = np.bincount(y_test)\n",
    "ax3.pie(class_counts, labels=[f'Class {i}' for i in range(len(class_counts))], \n",
    "        autopct='%1.1f%%', startangle=90, colors=['lightblue', 'lightgreen'])\n",
    "ax3.set_title('ğŸ¯ Test Set Class Distribution', fontweight='bold')\n",
    "\n",
    "# 4. Prediction Accuracy by Class\n",
    "ax4 = axes[1, 1]\n",
    "class_accuracies = []\n",
    "for class_label in [0, 1]:\n",
    "    mask = y_test == class_label\n",
    "    class_acc = accuracy_score(y_test[mask], y_pred_ensemble[mask])\n",
    "    class_accuracies.append(class_acc)\n",
    "\n",
    "bars = ax4.bar([f'Class {i}' for i in range(len(class_accuracies))], \n",
    "               class_accuracies, color=['lightblue', 'lightgreen'])\n",
    "ax4.set_title('ğŸ“Š Per-Class Accuracy', fontweight='bold')\n",
    "ax4.set_ylabel('Accuracy')\n",
    "ax4.set_ylim(0, 1)\n",
    "\n",
    "# Add value labels\n",
    "for bar, acc in zip(bars, class_accuracies):\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Visualization completed!\")\n",
    "print(\"ğŸ“Š Four comprehensive performance charts created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47bd0a7",
   "metadata": {},
   "source": [
    "## ğŸ§  Step 9: Key Insights and Learning Outcomes\n",
    "\n",
    "### ğŸ¯ **Why Ensemble Methods Excel**:\n",
    "\n",
    "#### ğŸ”¬ **Mathematical Foundation**:\n",
    "1. **Bias-Variance Decomposition**: \n",
    "   - Individual models: High variance OR high bias\n",
    "   - Ensemble: Reduces both through averaging and diversity\n",
    "\n",
    "2. **Central Limit Theorem**: \n",
    "   - As we combine more independent predictors\n",
    "   - The ensemble prediction approaches the true signal\n",
    "\n",
    "3. **Condorcet's Jury Theorem**: \n",
    "   - If each classifier is better than random (>50% accuracy)\n",
    "   - The majority vote will be better than any individual\n",
    "\n",
    "#### ğŸª **Practical Benefits**:\n",
    "1. **ğŸ›¡ï¸ Robustness**: Less sensitive to outliers and noise\n",
    "2. **ğŸ“ˆ Stability**: More consistent performance across datasets\n",
    "3. **ğŸ¯ Accuracy**: Often higher than best individual model\n",
    "4. **ğŸ”§ Flexibility**: Can combine any types of algorithms\n",
    "\n",
    "### ğŸ“ **When to Use Ensemble Methods**:\n",
    "- âœ… **High-stakes decisions**: Medical diagnosis, financial predictions\n",
    "- âœ… **Kaggle competitions**: Often winning solutions are ensembles\n",
    "- âœ… **Uncertain environments**: When no single algorithm dominates\n",
    "- âœ… **Diverse data**: Mixed feature types, complex patterns\n",
    "\n",
    "### âš ï¸ **Limitations to Consider**:\n",
    "- ğŸŒ **Computational Cost**: Training multiple models takes more time\n",
    "- ğŸ”§ **Complexity**: Harder to interpret than single models\n",
    "- ğŸ’¾ **Memory Usage**: Must store multiple models\n",
    "- ğŸ¯ **Diminishing Returns**: Adding more models doesn't always help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f8dcf5",
   "metadata": {},
   "source": [
    "## ğŸ¯ Final Summary and Next Steps\n",
    "\n",
    "### âœ… **What We Accomplished**:\n",
    "\n",
    "1. **ğŸ“Š Dataset Creation**: Mastered `make_classification` parameters\n",
    "2. **ğŸ¤– Algorithm Diversity**: Assembled four different expert algorithms\n",
    "3. **ğŸ—³ï¸ Democratic Voting**: Implemented hard voting ensemble\n",
    "4. **ğŸ“ˆ Performance Analysis**: Compared individual vs ensemble performance\n",
    "5. **ğŸ¨ Visualization**: Created comprehensive performance charts\n",
    "6. **ğŸ§  Theory Understanding**: Learned why ensembles work mathematically\n",
    "\n",
    "### ğŸš€ **Next Steps in Your Ensemble Journey**:\n",
    "\n",
    "1. **ğŸ”„ Try Soft Voting**: Change `voting='soft'` for probability-based decisions\n",
    "2. **ğŸŒ² Explore Bagging**: Random Forest, Extra Trees\n",
    "3. **âš¡ Learn Boosting**: AdaBoost, Gradient Boosting, XGBoost\n",
    "4. **ğŸ—ï¸ Advanced Stacking**: Multi-level ensemble architectures\n",
    "5. **ğŸ¯ Real Datasets**: Apply to actual business problems\n",
    "\n",
    "### ğŸ“ **Key Takeaways**:\n",
    "- **ğŸ§  Diversity is Key**: Different algorithms catch different patterns\n",
    "- **ğŸ—³ï¸ Democracy Works**: Majority voting often beats individual experts\n",
    "- **ğŸ“Š Measure Everything**: Always compare ensemble vs individuals\n",
    "- **âš–ï¸ Balance Complexity**: More models â‰  always better\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ Congratulations! You've mastered the fundamentals of ensemble learning and voting classifiers!**\n",
    "\n",
    "*Continue your journey with more advanced ensemble techniques...*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
