{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ed896cd",
   "metadata": {},
   "source": [
    "# 🎯 Ensemble Methods: Voting Classifier Fundamentals\n",
    "\n",
    "## 📚 Complete Guide to Ensemble Learning and Voting Classifiers\n",
    "\n",
    "### 📖 Table of Contents:\n",
    "1. **[🔬 Theory Introduction](#theory)** - Ensemble learning concepts\n",
    "2. **[📊 Dataset Creation](#dataset)** - Understanding make_classification parameters\n",
    "3. **[🤖 Individual Classifiers](#classifiers)** - Building base models\n",
    "4. **[🗳️ Voting Classifier](#voting)** - Combining classifiers for better performance\n",
    "5. **[📈 Performance Evaluation](#evaluation)** - Measuring ensemble effectiveness\n",
    "6. **[🧠 Key Insights](#insights)** - Understanding why ensembles work\n",
    "\n",
    "### 🎓 What You'll Learn:\n",
    "1. **Ensemble Theory**: Why multiple models outperform single models\n",
    "2. **Dataset Parameters**: Deep understanding of `make_classification` parameters\n",
    "3. **Base Classifiers**: Naive Bayes, Logistic Regression, Decision Trees, SVM\n",
    "4. **Voting Mechanisms**: Hard vs Soft voting strategies\n",
    "5. **Performance Analysis**: Comparing individual vs ensemble performance\n",
    "\n",
    "### 🧠 Core Concepts:\n",
    "- **Ensemble Learning**: Combining multiple models for improved predictions\n",
    "- **Voting Classifier**: Democratic approach to classification\n",
    "- **Bias-Variance Tradeoff**: How ensembles reduce both bias and variance\n",
    "- **Diversity**: Why different algorithms complement each other\n",
    "\n",
    "### ⏱️ Estimated Reading Time: 20-25 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2016a5be",
   "metadata": {},
   "source": [
    "## 🔬 Theory Introduction: Why Ensemble Methods Work\n",
    "\n",
    "### 🧠 The Fundamental Principle:\n",
    "**\"The wisdom of crowds\"** - Multiple experts making decisions together often outperform any single expert.\n",
    "\n",
    "### 🎯 Mathematical Foundation:\n",
    "\n",
    "If we have **n** classifiers, each with accuracy **p > 0.5**, the probability that the majority vote is correct:\n",
    "\n",
    "**P(majority correct) = Σ C(n,k) × p^k × (1-p)^(n-k)**\n",
    "\n",
    "Where k > n/2 (majority)\n",
    "\n",
    "### 📊 Why This Works:\n",
    "1. **Error Reduction**: Different models make different types of errors\n",
    "2. **Variance Reduction**: Averaging reduces prediction variance\n",
    "3. **Bias Mitigation**: Combining diverse models can reduce systematic bias\n",
    "4. **Robustness**: Less sensitive to outliers and noise\n",
    "\n",
    "### 🔍 Types of Ensemble Methods:\n",
    "1. **Voting/Averaging**: Simple combination of predictions\n",
    "2. **Bagging**: Bootstrap Aggregating (Random Forest)\n",
    "3. **Boosting**: Sequential learning (AdaBoost, XGBoost)\n",
    "4. **Stacking**: Meta-learning approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dc41e6",
   "metadata": {},
   "source": [
    "## 📦 Step 1: Import Required Libraries\n",
    "\n",
    "We'll use scikit-learn's comprehensive suite for ensemble learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8332158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries for ensemble learning\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Dataset creation and preprocessing\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Individual classifiers (our \"experts\")\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Ensemble methods\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")\n",
    "print(\"🎯 Ready to explore ensemble learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e98770",
   "metadata": {},
   "source": [
    "## 📊 Step 2: Dataset Creation with make_classification\n",
    "\n",
    "### 🧮 Understanding make_classification Parameters:\n",
    "\n",
    "Let's create a synthetic dataset and understand every parameter in detail:\n",
    "\n",
    "#### 🔍 Parameter Deep Dive:\n",
    "\n",
    "1. **`n_samples=1000`**: \n",
    "   - **What**: Total number of data points to generate\n",
    "   - **Why**: 1000 provides sufficient data for reliable training and testing\n",
    "   - **Impact**: More samples = more robust model training\n",
    "\n",
    "2. **`n_features=20`**: \n",
    "   - **What**: Number of input features (dimensions)\n",
    "   - **Why**: 20 features create moderate complexity without curse of dimensionality\n",
    "   - **Impact**: More features = more complex decision boundaries\n",
    "\n",
    "3. **`n_classes=2`**: \n",
    "   - **What**: Number of target classes (binary classification)\n",
    "   - **Why**: Binary problems are easier to understand and visualize\n",
    "   - **Options**: Can be 2, 3, 4, etc. for multi-class problems\n",
    "   - **Impact**: More classes = more complex classification task\n",
    "\n",
    "4. **`random_state=1`**: \n",
    "   - **What**: Seed for random number generator\n",
    "   - **Why**: Ensures reproducible results across runs\n",
    "   - **Impact**: Same seed = identical dataset every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bdb513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎲 Create synthetic classification dataset\n",
    "print(\"🔬 Creating Synthetic Classification Dataset\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Generate the dataset with detailed parameters\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,        # Total number of samples\n",
    "    n_features=20,         # Number of input features\n",
    "    n_classes=2,           # Binary classification (0 and 1)\n",
    "    n_informative=15,      # Number of informative features\n",
    "    n_redundant=3,         # Number of redundant features\n",
    "    n_clusters_per_class=1, # Number of clusters per class\n",
    "    class_sep=1.0,         # Factor multiplying the hypercube size\n",
    "    random_state=1         # For reproducibility\n",
    ")\n",
    "\n",
    "print(f\"📊 Dataset Created Successfully!\")\n",
    "print(f\"📏 Feature matrix shape: {X.shape}\")\n",
    "print(f\"🎯 Target vector shape: {y.shape}\")\n",
    "print(f\"📈 Features: {X.shape[1]} dimensions\")\n",
    "print(f\"🔢 Samples: {X.shape[0]} data points\")\n",
    "print(f\"🏷️ Classes: {len(np.unique(y))} unique classes\")\n",
    "print(f\"📊 Class distribution: {np.bincount(y)}\")\n",
    "print(f\"⚖️ Class balance: {np.bincount(y) / len(y) * 100}%\")\n",
    "\n",
    "# Additional dataset insights\n",
    "print(\"\\n🔍 Dataset Characteristics:\")\n",
    "print(f\"📊 Feature value range: [{X.min():.2f}, {X.max():.2f}]\")\n",
    "print(f\"📈 Feature mean: {X.mean():.2f}\")\n",
    "print(f\"📉 Feature std: {X.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b388aca",
   "metadata": {},
   "source": [
    "### 🧠 Advanced make_classification Parameters Explained:\n",
    "\n",
    "#### 🎯 **n_informative vs n_redundant vs n_repeated**:\n",
    "\n",
    "1. **`n_informative=15`**: \n",
    "   - **Features that actually help** in classification\n",
    "   - These features have **genuine predictive power**\n",
    "   - **Example**: In medical diagnosis, these would be symptoms that actually indicate disease\n",
    "\n",
    "2. **`n_redundant=3`**: \n",
    "   - **Linear combinations** of informative features\n",
    "   - Add **multicollinearity** but no new information\n",
    "   - **Example**: If height and weight are informative, BMI would be redundant\n",
    "\n",
    "3. **`n_repeated=0`** (default): \n",
    "   - **Exact duplicates** of existing features\n",
    "   - Pure noise that adds no value\n",
    "\n",
    "#### 🎨 **class_sep Parameter**:\n",
    "- **What**: Controls how **separable** the classes are\n",
    "- **Range**: Higher values = easier separation\n",
    "- **Impact**: class_sep=0.5 → overlapping classes, class_sep=2.0 → clearly separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9d8307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔪 Split the dataset for training and testing\n",
    "print(\"🔀 Splitting Dataset for Training and Testing\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,                    # Our features and targets\n",
    "    test_size=0.2,          # 20% for testing, 80% for training\n",
    "    random_state=1,         # Reproducible splits\n",
    "    stratify=y              # Maintain class proportions\n",
    ")\n",
    "\n",
    "print(f\"🎓 Training set shape: {X_train.shape}\")\n",
    "print(f\"🧪 Testing set shape: {X_test.shape}\")\n",
    "print(f\"📊 Training samples: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"🔬 Testing samples: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Verify stratification worked\n",
    "print(f\"\\n⚖️ Class Distribution Verification:\")\n",
    "print(f\"📈 Original: {np.bincount(y) / len(y) * 100}%\")\n",
    "print(f\"🎓 Training: {np.bincount(y_train) / len(y_train) * 100}%\")\n",
    "print(f\"🧪 Testing: {np.bincount(y_test) / len(y_test) * 100}%\")\n",
    "print(\"✅ Stratification successful - proportions maintained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffa8fd1",
   "metadata": {},
   "source": [
    "## 🤖 Step 3: Individual Classifiers - Our \"Expert Panel\"\n",
    "\n",
    "### 🧠 Why These Four Algorithms?\n",
    "\n",
    "We're selecting four fundamentally different algorithms to maximize **diversity**:\n",
    "\n",
    "#### 🎯 **Algorithm Diversity Analysis**:\n",
    "\n",
    "1. **🔢 Naive Bayes (Probabilistic)**:\n",
    "   - **Assumption**: Features are independent given the class\n",
    "   - **Strength**: Fast, works well with small datasets\n",
    "   - **Weakness**: Independence assumption often violated\n",
    "   - **Best for**: Text classification, categorical features\n",
    "\n",
    "2. **📈 Logistic Regression (Linear)**:\n",
    "   - **Assumption**: Linear relationship between features and log-odds\n",
    "   - **Strength**: Interpretable, provides probabilities\n",
    "   - **Weakness**: Assumes linear decision boundary\n",
    "   - **Best for**: Linear separable problems, feature importance\n",
    "\n",
    "3. **🌳 Decision Tree (Non-linear)**:\n",
    "   - **Assumption**: Data can be split using feature thresholds\n",
    "   - **Strength**: Handles non-linear patterns, interpretable\n",
    "   - **Weakness**: Prone to overfitting, unstable\n",
    "   - **Best for**: Non-linear patterns, mixed data types\n",
    "\n",
    "4. **🎯 SVM (Geometric)**:\n",
    "   - **Assumption**: Optimal separating hyperplane exists\n",
    "   - **Strength**: Effective in high dimensions, memory efficient\n",
    "   - **Weakness**: Slow on large datasets, requires feature scaling\n",
    "   - **Best for**: High-dimensional data, robust classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e507a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🤖 Initialize our \"Expert Panel\" of classifiers\n",
    "print(\"🤖 Assembling Expert Panel of Classifiers\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "# Create individual classifiers with optimal parameters\n",
    "print(\"🔢 Expert 1: Naive Bayes (Probabilistic Approach)\")\n",
    "nb_clf = GaussianNB()\n",
    "print(\"   📊 Assumes: Feature independence given class\")\n",
    "print(\"   🎯 Strength: Fast, probabilistic outputs\")\n",
    "\n",
    "print(\"\\n📈 Expert 2: Logistic Regression (Linear Approach)\")\n",
    "lr_clf = LogisticRegression(random_state=1, max_iter=1000)\n",
    "print(\"   📊 Assumes: Linear decision boundary\")\n",
    "print(\"   🎯 Strength: Interpretable, stable\")\n",
    "\n",
    "print(\"\\n🌳 Expert 3: Decision Tree (Rule-based Approach)\")\n",
    "dt_clf = DecisionTreeClassifier(random_state=1, max_depth=10)\n",
    "print(\"   📊 Assumes: Hierarchical feature splits\")\n",
    "print(\"   🎯 Strength: Non-linear, interpretable\")\n",
    "\n",
    "print(\"\\n🎯 Expert 4: Support Vector Machine (Geometric Approach)\")\n",
    "svm_clf = SVC(kernel='linear', random_state=1)\n",
    "print(\"   📊 Assumes: Optimal separating hyperplane\")\n",
    "print(\"   🎯 Strength: Robust, high-dimensional\")\n",
    "\n",
    "print(\"\\n✅ Expert Panel Assembled!\")\n",
    "print(\"🎭 Four diverse algorithms ready for ensemble\")\n",
    "\n",
    "# Store classifiers for easy reference\n",
    "classifiers = {\n",
    "    'Naive Bayes': nb_clf,\n",
    "    'Logistic Regression': lr_clf,\n",
    "    'Decision Tree': dt_clf,\n",
    "    'SVM': svm_clf\n",
    "}\n",
    "\n",
    "print(f\"\\n📋 Expert Panel Summary: {len(classifiers)} diverse algorithms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd28306",
   "metadata": {},
   "source": [
    "## 🗳️ Step 4: The Voting Classifier - Democratic Decision Making\n",
    "\n",
    "### 🧠 Voting Mechanisms Explained:\n",
    "\n",
    "#### 🗳️ **Hard Voting (Majority Rule)**:\n",
    "- **Process**: Each classifier gives a class prediction (0 or 1)\n",
    "- **Decision**: Majority vote wins\n",
    "- **Example**: \n",
    "  - NB predicts: 1, LR predicts: 0, DT predicts: 1, SVM predicts: 1\n",
    "  - **Result**: Class 1 (3 votes vs 1 vote)\n",
    "\n",
    "#### 🎯 **Soft Voting (Probability-based)**:\n",
    "- **Process**: Each classifier gives probability estimates\n",
    "- **Decision**: Average probabilities, choose highest\n",
    "- **Example**: \n",
    "  - NB: [0.2, 0.8], LR: [0.6, 0.4], DT: [0.3, 0.7], SVM: [0.4, 0.6]\n",
    "  - **Average**: [0.375, 0.625] → **Result**: Class 1\n",
    "\n",
    "#### 🎪 **Why Voting Works**:\n",
    "1. **Error Compensation**: Different algorithms make different mistakes\n",
    "2. **Confidence Weighting**: More confident predictions have more impact\n",
    "3. **Stability**: Reduces variance of individual predictions\n",
    "4. **Robustness**: Less sensitive to outliers or noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58708a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🗳️ Create the Voting Classifier - Our Democratic Ensemble\n",
    "print(\"🗳️ Creating Democratic Voting Classifier\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Define the ensemble with descriptive names\n",
    "ensemble_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('naive_bayes', nb_clf),           # Probabilistic expert\n",
    "        ('logistic_regression', lr_clf),   # Linear expert\n",
    "        ('decision_tree', dt_clf),         # Non-linear expert\n",
    "        ('svm', svm_clf)                   # Geometric expert\n",
    "    ],\n",
    "    voting='hard'  # Majority voting (can be 'soft' for probability averaging)\n",
    ")\n",
    "\n",
    "print(f\"🎭 Ensemble Configuration:\")\n",
    "print(f\"   👥 Number of experts: {len(ensemble_clf.estimators)}\")\n",
    "print(f\"   🗳️ Voting method: {ensemble_clf.voting.upper()}\")\n",
    "print(f\"   🎯 Decision rule: Majority vote wins\")\n",
    "\n",
    "print(f\"\\n📋 Expert Panel Members:\")\n",
    "for name, clf in ensemble_clf.estimators:\n",
    "    print(f\"   🤖 {name}: {type(clf).__name__}\")\n",
    "\n",
    "print(f\"\\n🧠 Voting Logic:\")\n",
    "print(f\"   📊 Each expert gives class prediction (0 or 1)\")\n",
    "print(f\"   🗳️ Final prediction = majority vote\")\n",
    "print(f\"   🎯 Ties resolved by first classifier in order\")\n",
    "\n",
    "print(f\"\\n✅ Democratic Ensemble Ready!\")\n",
    "print(f\"🎪 Four diverse experts working together\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8cbd9d",
   "metadata": {},
   "source": [
    "### 🔬 Training Phase: Teaching Our Expert Panel\n",
    "\n",
    "When we train the voting classifier, here's what happens:\n",
    "\n",
    "1. **🎓 Individual Training**: Each base classifier learns from the training data\n",
    "2. **🧠 Pattern Learning**: Each algorithm discovers different patterns\n",
    "3. **🎯 Specialization**: Each expert becomes good at different aspects\n",
    "4. **🤝 Integration**: Voting mechanism prepares to combine their expertise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7df4ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎓 Train the ensemble - Teaching our expert panel\n",
    "print(\"🎓 Training the Expert Panel\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "print(\"📚 Training individual experts...\")\n",
    "start_time = np.datetime64('now')\n",
    "\n",
    "# Train the ensemble (automatically trains all base classifiers)\n",
    "ensemble_clf.fit(X_train, y_train)\n",
    "\n",
    "end_time = np.datetime64('now')\n",
    "print(f\"⏱️ Training completed!\")\n",
    "\n",
    "print(f\"\\n🎯 Training Summary:\")\n",
    "print(f\"   📊 Training samples: {len(X_train)}\")\n",
    "print(f\"   🔢 Features per sample: {X_train.shape[1]}\")\n",
    "print(f\"   🎭 Experts trained: {len(ensemble_clf.estimators)}\")\n",
    "\n",
    "print(f\"\\n✅ Expert Panel Successfully Trained!\")\n",
    "print(f\"🤖 All {len(ensemble_clf.estimators)} experts ready for predictions\")\n",
    "print(f\"🗳️ Democratic voting mechanism activated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f784641",
   "metadata": {},
   "source": [
    "## 📈 Step 5: Making Predictions - The Democratic Process\n",
    "\n",
    "### 🗳️ How Ensemble Prediction Works:\n",
    "\n",
    "For each test sample:\n",
    "1. **🤖 Individual Predictions**: Each expert makes its prediction\n",
    "2. **🗳️ Vote Collection**: Gather all expert opinions\n",
    "3. **📊 Vote Counting**: Count votes for each class\n",
    "4. **🎯 Final Decision**: Majority vote determines final prediction\n",
    "\n",
    "**Example Process:**\n",
    "- Sample X: [feature1=2.1, feature2=-0.5, ...]\n",
    "- Expert votes: NB→1, LR→0, DT→1, SVM→1\n",
    "- Vote count: Class 0 = 1 vote, Class 1 = 3 votes\n",
    "- **Final prediction: Class 1** ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6330f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔮 Make predictions using our democratic ensemble\n",
    "print(\"🔮 Making Predictions with Democratic Ensemble\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "print(\"🗳️ Collecting votes from expert panel...\")\n",
    "\n",
    "# Get ensemble predictions\n",
    "y_pred_ensemble = ensemble_clf.predict(X_test)\n",
    "\n",
    "print(f\"✅ Predictions completed!\")\n",
    "print(f\"   📊 Test samples processed: {len(X_test)}\")\n",
    "print(f\"   🎭 Experts consulted per sample: {len(ensemble_clf.estimators)}\")\n",
    "print(f\"   🗳️ Total votes cast: {len(X_test) * len(ensemble_clf.estimators)}\")\n",
    "\n",
    "# Show sample predictions\n",
    "print(f\"\\n📋 Sample Predictions (first 10):\")\n",
    "print(f\"   🔮 Ensemble: {y_pred_ensemble[:10]}\")\n",
    "print(f\"   🎯 Actual:   {y_test[:10]}\")\n",
    "\n",
    "# Quick accuracy check\n",
    "quick_accuracy = accuracy_score(y_test, y_pred_ensemble)\n",
    "print(f\"\\n🎯 Quick Accuracy Check: {quick_accuracy:.4f} ({quick_accuracy*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705867c8",
   "metadata": {},
   "source": [
    "## 📊 Step 6: Individual Expert Performance Analysis\n",
    "\n",
    "Let's see how each expert performs individually before combining them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93c6e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Evaluate individual expert performance\n",
    "print(\"📊 Individual Expert Performance Analysis\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "individual_scores = {}\n",
    "\n",
    "print(\"🔍 Testing each expert individually...\")\n",
    "print()\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    # Train individual classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_individual = clf.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred_individual)\n",
    "    individual_scores[name] = accuracy\n",
    "    \n",
    "    print(f\"🤖 {name}:\")\n",
    "    print(f\"   🎯 Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print()\n",
    "\n",
    "# Calculate ensemble accuracy\n",
    "ensemble_accuracy = accuracy_score(y_test, y_pred_ensemble)\n",
    "individual_scores['Ensemble'] = ensemble_accuracy\n",
    "\n",
    "print(f\"🗳️ Democratic Ensemble:\")\n",
    "print(f\"   🎯 Accuracy: {ensemble_accuracy:.4f} ({ensemble_accuracy*100:.2f}%)\")\n",
    "print()\n",
    "\n",
    "# Find best individual performer\n",
    "best_individual = max(individual_scores.items(), key=lambda x: x[1] if x[0] != 'Ensemble' else 0)\n",
    "print(f\"🏆 Best Individual Expert: {best_individual[0]} ({best_individual[1]*100:.2f}%)\")\n",
    "\n",
    "# Calculate improvement\n",
    "improvement = ensemble_accuracy - best_individual[1]\n",
    "print(f\"📈 Ensemble Improvement: {improvement:+.4f} ({improvement*100:+.2f} percentage points)\")\n",
    "\n",
    "if improvement > 0:\n",
    "    print(\"✅ Ensemble outperforms best individual expert!\")\n",
    "else:\n",
    "    print(\"⚠️ Individual expert outperforms ensemble (possible overfitting)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e191fd7",
   "metadata": {},
   "source": [
    "## 📈 Step 7: Comprehensive Performance Evaluation\n",
    "\n",
    "### 🎯 Evaluation Metrics Explained:\n",
    "\n",
    "1. **📊 Accuracy**: Overall correctness (TP+TN)/(TP+TN+FP+FN)\n",
    "2. **🎯 Precision**: How many predicted positives were actually positive\n",
    "3. **📈 Recall**: How many actual positives were correctly identified\n",
    "4. **⚖️ F1-Score**: Harmonic mean of precision and recall\n",
    "5. **🎪 Confusion Matrix**: Detailed breakdown of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4fe4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📈 Comprehensive performance evaluation\n",
    "print(\"📈 Comprehensive Ensemble Performance Evaluation\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Calculate final accuracy\n",
    "final_accuracy = accuracy_score(y_test, y_pred_ensemble)\n",
    "print(f\"🎯 Final Ensemble Accuracy: {final_accuracy:.4f} ({final_accuracy*100:.2f}%)\")\n",
    "print()\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"📊 Detailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_ensemble, target_names=['Class 0', 'Class 1']))\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"🎪 Confusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred_ensemble)\n",
    "print(cm)\n",
    "print()\n",
    "\n",
    "# Confusion matrix interpretation\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"🔍 Confusion Matrix Breakdown:\")\n",
    "print(f\"   ✅ True Negatives (Correct Class 0): {tn}\")\n",
    "print(f\"   ❌ False Positives (Wrong Class 1): {fp}\")\n",
    "print(f\"   ❌ False Negatives (Missed Class 1): {fn}\")\n",
    "print(f\"   ✅ True Positives (Correct Class 1): {tp}\")\n",
    "print()\n",
    "\n",
    "# Calculate additional metrics\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(f\"📊 Manual Metric Verification:\")\n",
    "print(f\"   🎯 Precision: {precision:.4f}\")\n",
    "print(f\"   📈 Recall: {recall:.4f}\")\n",
    "print(f\"   ⚖️ F1-Score: {f1:.4f}\")\n",
    "print(f\"   📊 Accuracy: {(tp + tn) / (tp + tn + fp + fn):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6e4393",
   "metadata": {},
   "source": [
    "## 🎨 Step 8: Performance Visualization\n",
    "\n",
    "Let's create visualizations to better understand our ensemble's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a409df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎨 Performance visualization\n",
    "print(\"🎨 Creating Performance Visualizations\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('🗳️ Ensemble Learning Performance Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Individual vs Ensemble Accuracy Comparison\n",
    "ax1 = axes[0, 0]\n",
    "names = list(individual_scores.keys())\n",
    "scores = list(individual_scores.values())\n",
    "colors = ['skyblue', 'lightgreen', 'lightcoral', 'lightsalmon', 'gold']\n",
    "\n",
    "bars = ax1.bar(names, scores, color=colors)\n",
    "ax1.set_title('🏆 Individual vs Ensemble Performance', fontweight='bold')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars, scores):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 2. Confusion Matrix Heatmap\n",
    "ax2 = axes[0, 1]\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Predicted 0', 'Predicted 1'],\n",
    "            yticklabels=['Actual 0', 'Actual 1'], ax=ax2)\n",
    "ax2.set_title('🎪 Confusion Matrix', fontweight='bold')\n",
    "\n",
    "# 3. Class Distribution\n",
    "ax3 = axes[1, 0]\n",
    "class_counts = np.bincount(y_test)\n",
    "ax3.pie(class_counts, labels=[f'Class {i}' for i in range(len(class_counts))], \n",
    "        autopct='%1.1f%%', startangle=90, colors=['lightblue', 'lightgreen'])\n",
    "ax3.set_title('🎯 Test Set Class Distribution', fontweight='bold')\n",
    "\n",
    "# 4. Prediction Accuracy by Class\n",
    "ax4 = axes[1, 1]\n",
    "class_accuracies = []\n",
    "for class_label in [0, 1]:\n",
    "    mask = y_test == class_label\n",
    "    class_acc = accuracy_score(y_test[mask], y_pred_ensemble[mask])\n",
    "    class_accuracies.append(class_acc)\n",
    "\n",
    "bars = ax4.bar([f'Class {i}' for i in range(len(class_accuracies))], \n",
    "               class_accuracies, color=['lightblue', 'lightgreen'])\n",
    "ax4.set_title('📊 Per-Class Accuracy', fontweight='bold')\n",
    "ax4.set_ylabel('Accuracy')\n",
    "ax4.set_ylim(0, 1)\n",
    "\n",
    "# Add value labels\n",
    "for bar, acc in zip(bars, class_accuracies):\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Visualization completed!\")\n",
    "print(\"📊 Four comprehensive performance charts created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47bd0a7",
   "metadata": {},
   "source": [
    "## 🧠 Step 9: Key Insights and Learning Outcomes\n",
    "\n",
    "### 🎯 **Why Ensemble Methods Excel**:\n",
    "\n",
    "#### 🔬 **Mathematical Foundation**:\n",
    "1. **Bias-Variance Decomposition**: \n",
    "   - Individual models: High variance OR high bias\n",
    "   - Ensemble: Reduces both through averaging and diversity\n",
    "\n",
    "2. **Central Limit Theorem**: \n",
    "   - As we combine more independent predictors\n",
    "   - The ensemble prediction approaches the true signal\n",
    "\n",
    "3. **Condorcet's Jury Theorem**: \n",
    "   - If each classifier is better than random (>50% accuracy)\n",
    "   - The majority vote will be better than any individual\n",
    "\n",
    "#### 🎪 **Practical Benefits**:\n",
    "1. **🛡️ Robustness**: Less sensitive to outliers and noise\n",
    "2. **📈 Stability**: More consistent performance across datasets\n",
    "3. **🎯 Accuracy**: Often higher than best individual model\n",
    "4. **🔧 Flexibility**: Can combine any types of algorithms\n",
    "\n",
    "### 🎓 **When to Use Ensemble Methods**:\n",
    "- ✅ **High-stakes decisions**: Medical diagnosis, financial predictions\n",
    "- ✅ **Kaggle competitions**: Often winning solutions are ensembles\n",
    "- ✅ **Uncertain environments**: When no single algorithm dominates\n",
    "- ✅ **Diverse data**: Mixed feature types, complex patterns\n",
    "\n",
    "### ⚠️ **Limitations to Consider**:\n",
    "- 🐌 **Computational Cost**: Training multiple models takes more time\n",
    "- 🔧 **Complexity**: Harder to interpret than single models\n",
    "- 💾 **Memory Usage**: Must store multiple models\n",
    "- 🎯 **Diminishing Returns**: Adding more models doesn't always help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f8dcf5",
   "metadata": {},
   "source": [
    "## 🎯 Final Summary and Next Steps\n",
    "\n",
    "### ✅ **What We Accomplished**:\n",
    "\n",
    "1. **📊 Dataset Creation**: Mastered `make_classification` parameters\n",
    "2. **🤖 Algorithm Diversity**: Assembled four different expert algorithms\n",
    "3. **🗳️ Democratic Voting**: Implemented hard voting ensemble\n",
    "4. **📈 Performance Analysis**: Compared individual vs ensemble performance\n",
    "5. **🎨 Visualization**: Created comprehensive performance charts\n",
    "6. **🧠 Theory Understanding**: Learned why ensembles work mathematically\n",
    "\n",
    "### 🚀 **Next Steps in Your Ensemble Journey**:\n",
    "\n",
    "1. **🔄 Try Soft Voting**: Change `voting='soft'` for probability-based decisions\n",
    "2. **🌲 Explore Bagging**: Random Forest, Extra Trees\n",
    "3. **⚡ Learn Boosting**: AdaBoost, Gradient Boosting, XGBoost\n",
    "4. **🏗️ Advanced Stacking**: Multi-level ensemble architectures\n",
    "5. **🎯 Real Datasets**: Apply to actual business problems\n",
    "\n",
    "### 🎓 **Key Takeaways**:\n",
    "- **🧠 Diversity is Key**: Different algorithms catch different patterns\n",
    "- **🗳️ Democracy Works**: Majority voting often beats individual experts\n",
    "- **📊 Measure Everything**: Always compare ensemble vs individuals\n",
    "- **⚖️ Balance Complexity**: More models ≠ always better\n",
    "\n",
    "---\n",
    "\n",
    "**🎉 Congratulations! You've mastered the fundamentals of ensemble learning and voting classifiers!**\n",
    "\n",
    "*Continue your journey with more advanced ensemble techniques...*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
